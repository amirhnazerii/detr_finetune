{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7926fc3-6925-43a0-8900-9f97e5cedc56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from engine import evaluate, train_one_epoch\n",
    "from models import build_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7315bd1a-ecc2-449b-b2f3-a6aec6c2f7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--num_classes', type=int, default=None,\n",
    "                        help=\"Number of classes in dataset+1\")\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "\n",
    "    \n",
    "    ####################### @amirhnazerii #######################\n",
    "    ##### start 03/27/2025\n",
    "    # * Classification head\n",
    "    parser.add_argument('--new_layer_dim', default=None, type=int,\n",
    "                        help=\"classification head added fc-layer dim\")\n",
    "    \n",
    "    ##### end 03/27/2025\n",
    "    \n",
    "    \n",
    "    # * Segmentation\n",
    "    parser.add_argument('--masks', action='store_true',\n",
    "                        help=\"Train segmentation head if the flag is provided\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--inter_class_weight', default=None, type=float,\n",
    "                    help=\"Weight for inter-class distance maximization in CenterLoss - amirhnazerii 5/2/2025\")\n",
    "\n",
    "    \n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str)\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "    \n",
    "    \n",
    "    # robustness param:\n",
    "    parser.add_argument('--robust', default=False, type=bool,\n",
    "                        help='nrobust detr training with modified loss function.')\n",
    "    \n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34599056-9fa9-46a2-95f3-43e84df5d3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n",
    "args = parser.parse_args(args=['--resume', 'detr-r50-modifhead-128fc92fc.pth',\n",
    "                              '--coco_path', '/scratch/anazeri/coco/',\n",
    "                               '--output_dir' ,'/home/anazeri/detr_finetune/robust-detr-r50-coco-modifhead-128fc92fc-TEMP',\n",
    "                               \"--lr_drop\", \"7\",  \n",
    "                              '--new_layer_dim', '128',\n",
    "                               \"--inter_class_weight\",\"0\",\n",
    "                              \"--robust\", \"True\",\n",
    "                               \"--epochs\", \"10\"\n",
    "                              ])\n",
    "\n",
    "args.distributed = False\n",
    "# --dataset_file \"coco\" \\\n",
    "    # --coco_path \"/scratch/anazeri/coco/\" \\\n",
    "    # --output_dir \"/home/anazeri/detr_finetune/detr-r50-coco-modifhead-128fc92fc-epoch10\" \\\n",
    "    # --resume \"detr-r50-modifhead-128fc92fc.pth\" \\\n",
    "    # --lr_drop 3 \\\n",
    "    # --backbone \"resnet50\" \\\n",
    "    # --epochs 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c208619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=10, lr_drop=7, clip_max_norm=0.1, num_classes=None, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, pre_norm=False, new_layer_dim=128, masks=False, aux_loss=True, inter_class_weight=0.0, set_cost_class=1, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, dataset_file='coco', coco_path='/scratch/anazeri/coco/', coco_panoptic_path=None, remove_difficult=False, output_dir='/home/anazeri/detr_finetune/robust-detr-r50-coco-modifhead-128fc92fc-TEMP', device='cuda', seed=42, resume='detr-r50-modifhead-128fc92fc.pth', start_epoch=0, eval=False, num_workers=2, world_size=1, dist_url='env://', robust=True, distributed=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if args.world_size > 1:\n",
    "    utils.init_distributed_mode(args)  # Enable distributed mode if running on multiple GPUs\n",
    "    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "else:\n",
    "    args.distributed = False  # Force single GPU mode\n",
    "\n",
    "\n",
    "print(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7751ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 44764\n",
      "loading annotations into memory...\n",
      "Done (t=10.60s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.34s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "####################### @amirhnazerii #######################\n",
    "##### start 03/28/2025\n",
    "## Freeze all model parameters except the classification head\n",
    "for param in model_without_ddp.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_without_ddp.class_embed.parameters():\n",
    "    param.requires_grad = True  # Only train the classification head\n",
    "\n",
    "# Re-define optimizer (only updates classification head parameters)\n",
    "optimizer = torch.optim.AdamW(model_without_ddp.class_embed.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "#     optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "#                                   weight_decay=args.weight_decay)\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model_without_ddp.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "##### end   \n",
    "\n",
    "\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "dataset_train = build_dataset(image_set='train', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "if args.distributed:\n",
    "    sampler_train = DistributedSampler(dataset_train)\n",
    "    sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "else:\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                               collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                             drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if args.dataset_file == \"coco_panoptic\":\n",
    "    # We also evaluate AP during panoptic training, on original coco DS\n",
    "    coco_val = datasets.coco.build(\"val\", args)\n",
    "    base_ds = get_coco_api_from_dataset(coco_val)\n",
    "else:\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "output_dir = Path(args.output_dir)\n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model_without_ddp.load_state_dict(checkpoint['model'], strict=False)  #### Modified by Amir: , strict=False\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c72d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                   | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [    0/59143]  eta: 1 day, 14:40:04  lr: 0.000100  class_error: 100.00  loss: 26.8357 (26.8357)  loss_ce: 4.4853 (4.4853)  loss_center: 0.0153 (0.0153)  loss_ce_0: 4.4805 (4.4805)  loss_ce_1: 4.5219 (4.5219)  loss_ce_2: 4.4299 (4.4299)  loss_ce_3: 4.4502 (4.4502)  loss_ce_4: 4.4527 (4.4527)  loss_ce_unscaled: 4.4853 (4.4853)  class_error_unscaled: 100.0000 (100.0000)  loss_center_unscaled: 1.5257 (1.5257)  loss_ce_0_unscaled: 4.4805 (4.4805)  loss_center_0_unscaled: 1.4747 (1.4747)  loss_ce_1_unscaled: 4.5219 (4.5219)  loss_center_1_unscaled: 1.4726 (1.4726)  loss_ce_2_unscaled: 4.4299 (4.4299)  loss_center_2_unscaled: 1.4800 (1.4800)  loss_ce_3_unscaled: 4.4502 (4.4502)  loss_center_3_unscaled: 1.5418 (1.5418)  loss_ce_4_unscaled: 4.4527 (4.4527)  loss_center_4_unscaled: 1.5466 (1.5466)  time: 2.3537  data: 1.5298  max mem: 461\n",
      "Epoch: [0]  [   10/59143]  eta: 4:35:28  lr: 0.000100  class_error: 100.00  loss: 23.1441 (23.9475)  loss_ce: 3.9031 (4.0497)  loss_center: 0.0168 (0.0172)  loss_ce_0: 3.7628 (3.8721)  loss_ce_1: 3.8679 (3.9654)  loss_ce_2: 3.9102 (3.9960)  loss_ce_3: 3.8469 (4.0109)  loss_ce_4: 3.8615 (4.0361)  loss_ce_unscaled: 3.9031 (4.0497)  class_error_unscaled: 100.0000 (100.0000)  loss_center_unscaled: 1.6850 (1.7208)  loss_ce_0_unscaled: 3.7628 (3.8721)  loss_center_0_unscaled: 1.6589 (1.6550)  loss_ce_1_unscaled: 3.8679 (3.9654)  loss_center_1_unscaled: 1.7058 (1.7138)  loss_ce_2_unscaled: 3.9102 (3.9960)  loss_center_2_unscaled: 1.7298 (1.7101)  loss_ce_3_unscaled: 3.8469 (4.0109)  loss_center_3_unscaled: 1.7243 (1.7165)  loss_ce_4_unscaled: 3.8615 (4.0361)  loss_center_4_unscaled: 1.6995 (1.7135)  time: 0.2795  data: 0.1475  max mem: 773\n",
      "Epoch: [0]  [   20/59143]  eta: 2:58:51  lr: 0.000100  class_error: 100.00  loss: 22.6691 (22.8450)  loss_ce: 3.8290 (3.8825)  loss_center: 0.0179 (0.0175)  loss_ce_0: 3.6057 (3.6726)  loss_ce_1: 3.7327 (3.7648)  loss_ce_2: 3.7414 (3.8099)  loss_ce_3: 3.7932 (3.8315)  loss_ce_4: 3.8028 (3.8661)  loss_ce_unscaled: 3.8290 (3.8825)  class_error_unscaled: 100.0000 (100.0000)  loss_center_unscaled: 1.7933 (1.7510)  loss_ce_0_unscaled: 3.6057 (3.6726)  loss_center_0_unscaled: 1.6756 (1.6867)  loss_ce_1_unscaled: 3.7327 (3.7648)  loss_center_1_unscaled: 1.7456 (1.7398)  loss_ce_2_unscaled: 3.7414 (3.8099)  loss_center_2_unscaled: 1.7340 (1.7397)  loss_ce_3_unscaled: 3.7932 (3.8315)  loss_center_3_unscaled: 1.7500 (1.7471)  loss_ce_4_unscaled: 3.8028 (3.8661)  loss_center_4_unscaled: 1.7345 (1.7472)  time: 0.0729  data: 0.0082  max mem: 773\n",
      "Epoch: [0]  [   30/59143]  eta: 2:25:42  lr: 0.000100  class_error: 100.00  loss: 20.9324 (22.0612)  loss_ce: 3.5189 (3.7383)  loss_center: 0.0172 (0.0174)  loss_ce_0: 3.2826 (3.5517)  loss_ce_1: 3.4143 (3.6400)  loss_ce_2: 3.4809 (3.6837)  loss_ce_3: 3.5002 (3.7024)  loss_ce_4: 3.5456 (3.7276)  loss_ce_unscaled: 3.5189 (3.7383)  class_error_unscaled: 100.0000 (100.0000)  loss_center_unscaled: 1.7202 (1.7419)  loss_ce_0_unscaled: 3.2826 (3.5517)  loss_center_0_unscaled: 1.6842 (1.6859)  loss_ce_1_unscaled: 3.4143 (3.6400)  loss_center_1_unscaled: 1.6861 (1.7272)  loss_ce_2_unscaled: 3.4809 (3.6837)  loss_center_2_unscaled: 1.6871 (1.7297)  loss_ce_3_unscaled: 3.5002 (3.7024)  loss_center_3_unscaled: 1.7203 (1.7400)  loss_ce_4_unscaled: 3.5456 (3.7276)  loss_center_4_unscaled: 1.7317 (1.7421)  time: 0.0755  data: 0.0068  max mem: 773\n",
      "Epoch: [0]  [   40/59143]  eta: 2:08:04  lr: 0.000100  class_error: 100.00  loss: 19.3118 (21.1763)  loss_ce: 3.2799 (3.5885)  loss_center: 0.0170 (0.0176)  loss_ce_0: 3.1008 (3.4081)  loss_ce_1: 3.1857 (3.4931)  loss_ce_2: 3.2343 (3.5391)  loss_ce_3: 3.2588 (3.5544)  loss_ce_4: 3.2729 (3.5755)  loss_ce_unscaled: 3.2799 (3.5885)  class_error_unscaled: 100.0000 (100.0000)  loss_center_unscaled: 1.6975 (1.7578)  loss_ce_0_unscaled: 3.1008 (3.4081)  loss_center_0_unscaled: 1.6842 (1.6979)  loss_ce_1_unscaled: 3.1857 (3.4931)  loss_center_1_unscaled: 1.6719 (1.7368)  loss_ce_2_unscaled: 3.2343 (3.5391)  loss_center_2_unscaled: 1.6706 (1.7477)  loss_ce_3_unscaled: 3.2588 (3.5544)  loss_center_3_unscaled: 1.6913 (1.7521)  loss_ce_4_unscaled: 3.2729 (3.5755)  loss_center_4_unscaled: 1.7161 (1.7590)  time: 0.0759  data: 0.0065  max mem: 773\n",
      "Epoch: [0]  [   50/59143]  eta: 1:57:00  lr: 0.000100  class_error: 100.00  loss: 17.6550 (20.9516)  loss_ce: 3.0026 (3.5455)  loss_center: 0.0177 (0.0177)  loss_ce_0: 2.8759 (3.3847)  loss_ce_1: 2.8851 (3.4550)  loss_ce_2: 2.9967 (3.5017)  loss_ce_3: 3.0035 (3.5152)  loss_ce_4: 2.9835 (3.5318)  loss_ce_unscaled: 3.0026 (3.5455)  class_error_unscaled: 100.0000 (100.0000)  loss_center_unscaled: 1.7724 (1.7697)  loss_ce_0_unscaled: 2.8759 (3.3847)  loss_center_0_unscaled: 1.7076 (1.7114)  loss_ce_1_unscaled: 2.8851 (3.4550)  loss_center_1_unscaled: 1.7668 (1.7545)  loss_ce_2_unscaled: 2.9967 (3.5017)  loss_center_2_unscaled: 1.8391 (1.7663)  loss_ce_3_unscaled: 3.0035 (3.5152)  loss_center_3_unscaled: 1.7786 (1.7681)  loss_ce_4_unscaled: 2.9835 (3.5318)  loss_center_4_unscaled: 1.7646 (1.7723)  time: 0.0737  data: 0.0070  max mem: 784\n",
      "Epoch: [0]  [   60/59143]  eta: 1:49:45  lr: 0.000100  class_error: 100.00  loss: 17.5749 (20.1923)  loss_ce: 2.9736 (3.4137)  loss_center: 0.0173 (0.0178)  loss_ce_0: 2.8886 (3.2692)  loss_ce_1: 2.9274 (3.3329)  loss_ce_2: 2.9967 (3.3724)  loss_ce_3: 2.9838 (3.3848)  loss_ce_4: 2.9835 (3.4015)  loss_ce_unscaled: 2.9736 (3.4137)  class_error_unscaled: 100.0000 (100.0000)  loss_center_unscaled: 1.7338 (1.7811)  loss_ce_0_unscaled: 2.8886 (3.2692)  loss_center_0_unscaled: 1.7495 (1.7291)  loss_ce_1_unscaled: 2.9274 (3.3329)  loss_center_1_unscaled: 1.7538 (1.7618)  loss_ce_2_unscaled: 2.9967 (3.3724)  loss_center_2_unscaled: 1.7388 (1.7762)  loss_ce_3_unscaled: 2.9838 (3.3848)  loss_center_3_unscaled: 1.7695 (1.7786)  loss_ce_4_unscaled: 2.9835 (3.4015)  loss_center_4_unscaled: 1.7607 (1.7833)  time: 0.0734  data: 0.0070  max mem: 784\n",
      "Epoch: [0]  [   70/59143]  eta: 1:43:22  lr: 0.000100  class_error: 100.00  loss: 15.7388 (19.4019)  loss_ce: 2.6324 (3.2773)  loss_center: 0.0179 (0.0179)  loss_ce_0: 2.6433 (3.1483)  loss_ce_1: 2.5426 (3.2048)  loss_ce_2: 2.6603 (3.2394)  loss_ce_3: 2.6132 (3.2494)  loss_ce_4: 2.6272 (3.2647)  loss_ce_unscaled: 2.6324 (3.2773)  class_error_unscaled: 100.0000 (100.0000)  loss_center_unscaled: 1.7901 (1.7947)  loss_ce_0_unscaled: 2.6433 (3.1483)  loss_center_0_unscaled: 1.8428 (1.7488)  loss_ce_1_unscaled: 2.5426 (3.2048)  loss_center_1_unscaled: 1.7811 (1.7845)  loss_ce_2_unscaled: 2.6603 (3.2394)  loss_center_2_unscaled: 1.8022 (1.7965)  loss_ce_3_unscaled: 2.6132 (3.2494)  loss_center_3_unscaled: 1.7618 (1.7920)  loss_ce_4_unscaled: 2.6272 (3.2647)  loss_center_4_unscaled: 1.7775 (1.7982)  time: 0.0698  data: 0.0063  max mem: 784\n",
      "Epoch: [0]  [   80/59143]  eta: 1:38:43  lr: 0.000100  class_error: 100.00  loss: 13.7253 (18.6628)  loss_ce: 2.3224 (3.1494)  loss_center: 0.0200 (0.0182)  loss_ce_0: 2.2879 (3.0374)  loss_ce_1: 2.3098 (3.0841)  loss_ce_2: 2.2757 (3.1152)  loss_ce_3: 2.2654 (3.1221)  loss_ce_4: 2.3023 (3.1362)  loss_ce_unscaled: 2.3224 (3.1494)  class_error_unscaled: 100.0000 (99.9118)  loss_center_unscaled: 2.0008 (1.8240)  loss_ce_0_unscaled: 2.2879 (3.0374)  loss_center_0_unscaled: 1.9584 (1.7884)  loss_ce_1_unscaled: 2.3098 (3.0841)  loss_center_1_unscaled: 1.9834 (1.8253)  loss_ce_2_unscaled: 2.2757 (3.1152)  loss_center_2_unscaled: 2.0065 (1.8334)  loss_ce_3_unscaled: 2.2654 (3.1221)  loss_center_3_unscaled: 2.0284 (1.8233)  loss_ce_4_unscaled: 2.3023 (3.1362)  loss_center_4_unscaled: 2.0166 (1.8318)  time: 0.0662  data: 0.0064  max mem: 784\n",
      "Epoch: [0]  [   90/59143]  eta: 1:35:28  lr: 0.000100  class_error: 100.00  loss: 13.3657 (18.1023)  loss_ce: 2.2648 (3.0519)  loss_center: 0.0205 (0.0187)  loss_ce_0: 2.1805 (2.9527)  loss_ce_1: 2.1976 (2.9951)  loss_ce_2: 2.2185 (3.0200)  loss_ce_3: 2.2341 (3.0251)  loss_ce_4: 2.2498 (3.0388)  loss_ce_unscaled: 2.2648 (3.0519)  class_error_unscaled: 100.0000 (99.9215)  loss_center_unscaled: 2.0504 (1.8665)  loss_ce_0_unscaled: 2.1805 (2.9527)  loss_center_0_unscaled: 2.0347 (1.8343)  loss_ce_1_unscaled: 2.1976 (2.9951)  loss_center_1_unscaled: 2.0901 (1.8712)  loss_ce_2_unscaled: 2.2185 (3.0200)  loss_center_2_unscaled: 2.0925 (1.8709)  loss_ce_3_unscaled: 2.2341 (3.0251)  loss_center_3_unscaled: 2.0371 (1.8645)  loss_ce_4_unscaled: 2.2498 (3.0388)  loss_center_4_unscaled: 2.0636 (1.8736)  time: 0.0687  data: 0.0064  max mem: 797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  100/59143]  eta: 1:32:20  lr: 0.000100  class_error: 100.00  loss: 12.5042 (17.5914)  loss_ce: 2.1245 (2.9629)  loss_center: 0.0206 (0.0189)  loss_ce_0: 2.1612 (2.8766)  loss_ce_1: 2.0893 (2.9129)  loss_ce_2: 2.0527 (2.9331)  loss_ce_3: 2.0743 (2.9374)  loss_ce_4: 2.1054 (2.9496)  loss_ce_unscaled: 2.1245 (2.9629)  class_error_unscaled: 100.0000 (99.8862)  loss_center_unscaled: 2.0605 (1.8927)  loss_ce_0_unscaled: 2.1612 (2.8766)  loss_center_0_unscaled: 2.0557 (1.8630)  loss_ce_1_unscaled: 2.0893 (2.9129)  loss_center_1_unscaled: 2.0901 (1.8940)  loss_ce_2_unscaled: 2.0527 (2.9331)  loss_center_2_unscaled: 2.0925 (1.8962)  loss_ce_3_unscaled: 2.0743 (2.9374)  loss_center_3_unscaled: 2.0786 (1.8914)  loss_ce_4_unscaled: 2.1054 (2.9496)  loss_center_4_unscaled: 2.0636 (1.8986)  time: 0.0678  data: 0.0061  max mem: 797\n",
      "Epoch: [0]  [  110/59143]  eta: 1:29:46  lr: 0.000100  class_error: 100.00  loss: 13.2033 (17.2327)  loss_ce: 2.2166 (2.8992)  loss_center: 0.0207 (0.0191)  loss_ce_0: 2.1612 (2.8232)  loss_ce_1: 2.1433 (2.8572)  loss_ce_2: 2.2594 (2.8731)  loss_ce_3: 2.2043 (2.8744)  loss_ce_4: 2.2001 (2.8865)  loss_ce_unscaled: 2.2166 (2.8992)  class_error_unscaled: 100.0000 (99.8965)  loss_center_unscaled: 2.0677 (1.9135)  loss_ce_0_unscaled: 2.1612 (2.8232)  loss_center_0_unscaled: 2.0557 (1.8816)  loss_ce_1_unscaled: 2.1433 (2.8572)  loss_center_1_unscaled: 2.0782 (1.9135)  loss_ce_2_unscaled: 2.2594 (2.8731)  loss_center_2_unscaled: 2.0716 (1.9154)  loss_ce_3_unscaled: 2.2043 (2.8744)  loss_center_3_unscaled: 2.0786 (1.9127)  loss_ce_4_unscaled: 2.2001 (2.8865)  loss_center_4_unscaled: 2.0895 (1.9201)  time: 0.0651  data: 0.0067  max mem: 821\n",
      "Epoch: [0]  [  120/59143]  eta: 1:28:44  lr: 0.000100  class_error: 100.00  loss: 12.1073 (16.8211)  loss_ce: 2.0998 (2.8269)  loss_center: 0.0214 (0.0193)  loss_ce_0: 1.9765 (2.7604)  loss_ce_1: 1.9526 (2.7892)  loss_ce_2: 1.9865 (2.8052)  loss_ce_3: 2.0161 (2.8046)  loss_ce_4: 2.0774 (2.8154)  loss_ce_unscaled: 2.0998 (2.8269)  class_error_unscaled: 100.0000 (99.9050)  loss_center_unscaled: 2.1407 (1.9343)  loss_ce_0_unscaled: 1.9765 (2.7604)  loss_center_0_unscaled: 2.0761 (1.9073)  loss_ce_1_unscaled: 1.9526 (2.7892)  loss_center_1_unscaled: 2.1068 (1.9340)  loss_ce_2_unscaled: 1.9865 (2.8052)  loss_center_2_unscaled: 2.0846 (1.9350)  loss_ce_3_unscaled: 2.0161 (2.8046)  loss_center_3_unscaled: 2.0964 (1.9334)  loss_ce_4_unscaled: 2.0774 (2.8154)  loss_center_4_unscaled: 2.1243 (1.9406)  time: 0.0718  data: 0.0075  max mem: 821\n",
      "Epoch: [0]  [  130/59143]  eta: 1:27:21  lr: 0.000100  class_error: 100.00  loss: 10.8522 (16.3800)  loss_ce: 1.7768 (2.7498)  loss_center: 0.0212 (0.0195)  loss_ce_0: 1.8022 (2.6947)  loss_ce_1: 1.8046 (2.7173)  loss_ce_2: 1.8190 (2.7317)  loss_ce_3: 1.7890 (2.7288)  loss_ce_4: 1.7787 (2.7382)  loss_ce_unscaled: 1.7768 (2.7498)  class_error_unscaled: 100.0000 (99.8791)  loss_center_unscaled: 2.1238 (1.9529)  loss_ce_0_unscaled: 1.8022 (2.6947)  loss_center_0_unscaled: 2.1697 (1.9340)  loss_ce_1_unscaled: 1.8046 (2.7173)  loss_center_1_unscaled: 2.1526 (1.9549)  loss_ce_2_unscaled: 1.8190 (2.7317)  loss_center_2_unscaled: 2.1486 (1.9550)  loss_ce_3_unscaled: 1.7890 (2.7288)  loss_center_3_unscaled: 2.1432 (1.9521)  loss_ce_4_unscaled: 1.7787 (2.7382)  loss_center_4_unscaled: 2.1407 (1.9583)  time: 0.0754  data: 0.0073  max mem: 821\n",
      "Epoch: [0]  [  140/59143]  eta: 1:26:17  lr: 0.000100  class_error: 66.67  loss: 10.8631 (16.1648)  loss_ce: 1.7990 (2.7105)  loss_center: 0.0215 (0.0197)  loss_ce_0: 1.7641 (2.6657)  loss_ce_1: 1.8435 (2.6827)  loss_ce_2: 1.8278 (2.6951)  loss_ce_3: 1.7971 (2.6914)  loss_ce_4: 1.8084 (2.6997)  loss_ce_unscaled: 1.7990 (2.7105)  class_error_unscaled: 100.0000 (99.5159)  loss_center_unscaled: 2.1504 (1.9692)  loss_ce_0_unscaled: 1.7641 (2.6657)  loss_center_0_unscaled: 2.1847 (1.9531)  loss_ce_1_unscaled: 1.8435 (2.6827)  loss_center_1_unscaled: 2.1510 (1.9725)  loss_ce_2_unscaled: 1.8278 (2.6951)  loss_center_2_unscaled: 2.1720 (1.9723)  loss_ce_3_unscaled: 1.7971 (2.6914)  loss_center_3_unscaled: 2.1569 (1.9704)  loss_ce_4_unscaled: 1.8084 (2.6997)  loss_center_4_unscaled: 2.1407 (1.9750)  time: 0.0728  data: 0.0068  max mem: 821\n",
      "Epoch: [0]  [  150/59143]  eta: 1:24:37  lr: 0.000100  class_error: 90.00  loss: 10.7329 (15.7821)  loss_ce: 1.7428 (2.6438)  loss_center: 0.0216 (0.0199)  loss_ce_0: 1.7834 (2.6062)  loss_ce_1: 1.7817 (2.6213)  loss_ce_2: 1.7747 (2.6308)  loss_ce_3: 1.7363 (2.6263)  loss_ce_4: 1.7207 (2.6337)  loss_ce_unscaled: 1.7428 (2.6438)  class_error_unscaled: 100.0000 (99.1272)  loss_center_unscaled: 2.1628 (1.9906)  loss_ce_0_unscaled: 1.7834 (2.6062)  loss_center_0_unscaled: 2.1847 (1.9752)  loss_ce_1_unscaled: 1.7817 (2.6213)  loss_center_1_unscaled: 2.1776 (2.0009)  loss_ce_2_unscaled: 1.7747 (2.6308)  loss_center_2_unscaled: 2.1508 (1.9926)  loss_ce_3_unscaled: 1.7363 (2.6263)  loss_center_3_unscaled: 2.1661 (1.9921)  loss_ce_4_unscaled: 1.7207 (2.6337)  loss_center_4_unscaled: 2.1505 (1.9971)  time: 0.0680  data: 0.0068  max mem: 821\n",
      "Epoch: [0]  [  160/59143]  eta: 1:23:44  lr: 0.000100  class_error: 66.67  loss: 9.6761 (15.4267)  loss_ce: 1.6076 (2.5818)  loss_center: 0.0232 (0.0202)  loss_ce_0: 1.6584 (2.5517)  loss_ce_1: 1.6412 (2.5642)  loss_ce_2: 1.6129 (2.5704)  loss_ce_3: 1.6170 (2.5663)  loss_ce_4: 1.5978 (2.5722)  loss_ce_unscaled: 1.6076 (2.5818)  class_error_unscaled: 100.0000 (98.5781)  loss_center_unscaled: 2.3164 (2.0201)  loss_ce_0_unscaled: 1.6584 (2.5517)  loss_center_0_unscaled: 2.3906 (2.0080)  loss_ce_1_unscaled: 1.6412 (2.5642)  loss_center_1_unscaled: 2.3383 (2.0351)  loss_ce_2_unscaled: 1.6129 (2.5704)  loss_center_2_unscaled: 2.3021 (2.0198)  loss_ce_3_unscaled: 1.6170 (2.5663)  loss_center_3_unscaled: 2.3753 (2.0217)  loss_ce_4_unscaled: 1.5978 (2.5722)  loss_center_4_unscaled: 2.2872 (2.0252)  time: 0.0672  data: 0.0064  max mem: 821\n",
      "Epoch: [0]  [  170/59143]  eta: 1:22:45  lr: 0.000100  class_error: 100.00  loss: 9.5904 (15.2079)  loss_ce: 1.6076 (2.5450)  loss_center: 0.0241 (0.0204)  loss_ce_0: 1.6372 (2.5180)  loss_ce_1: 1.6099 (2.5266)  loss_ce_2: 1.5324 (2.5349)  loss_ce_3: 1.5816 (2.5280)  loss_ce_4: 1.5978 (2.5351)  loss_ce_unscaled: 1.6076 (2.5450)  class_error_unscaled: 96.5517 (98.2417)  loss_center_unscaled: 2.4089 (2.0378)  loss_ce_0_unscaled: 1.6372 (2.5180)  loss_center_0_unscaled: 2.3972 (2.0255)  loss_ce_1_unscaled: 1.6099 (2.5266)  loss_center_1_unscaled: 2.4052 (2.0479)  loss_ce_2_unscaled: 1.5324 (2.5349)  loss_center_2_unscaled: 2.4052 (2.0385)  loss_ce_3_unscaled: 1.5816 (2.5280)  loss_center_3_unscaled: 2.3965 (2.0372)  loss_ce_4_unscaled: 1.5978 (2.5351)  loss_center_4_unscaled: 2.4044 (2.0418)  time: 0.0702  data: 0.0059  max mem: 821\n",
      "Epoch: [0]  [  180/59143]  eta: 1:22:03  lr: 0.000100  class_error: 91.67  loss: 10.4469 (14.9268)  loss_ce: 1.7409 (2.4970)  loss_center: 0.0238 (0.0206)  loss_ce_0: 1.7884 (2.4732)  loss_ce_1: 1.7842 (2.4819)  loss_ce_2: 1.6821 (2.4876)  loss_ce_3: 1.7144 (2.4801)  loss_ce_4: 1.7122 (2.4864)  loss_ce_unscaled: 1.7409 (2.4970)  class_error_unscaled: 88.8889 (97.2035)  loss_center_unscaled: 2.3768 (2.0558)  loss_ce_0_unscaled: 1.7884 (2.4732)  loss_center_0_unscaled: 2.3536 (2.0429)  loss_ce_1_unscaled: 1.7842 (2.4819)  loss_center_1_unscaled: 2.3459 (2.0674)  loss_ce_2_unscaled: 1.6821 (2.4876)  loss_center_2_unscaled: 2.3148 (2.0575)  loss_ce_3_unscaled: 1.7144 (2.4801)  loss_center_3_unscaled: 2.3507 (2.0548)  loss_ce_4_unscaled: 1.7122 (2.4864)  loss_center_4_unscaled: 2.3904 (2.0596)  time: 0.0700  data: 0.0064  max mem: 821\n",
      "Epoch: [0]  [  190/59143]  eta: 1:21:53  lr: 0.000100  class_error: 80.00  loss: 9.8446 (14.6847)  loss_ce: 1.6598 (2.4542)  loss_center: 0.0242 (0.0208)  loss_ce_0: 1.6259 (2.4367)  loss_ce_1: 1.6618 (2.4425)  loss_ce_2: 1.6041 (2.4475)  loss_ce_3: 1.6742 (2.4384)  loss_ce_4: 1.6686 (2.4446)  loss_ce_unscaled: 1.6598 (2.4542)  class_error_unscaled: 80.0000 (96.5514)  loss_center_unscaled: 2.4205 (2.0828)  loss_ce_0_unscaled: 1.6259 (2.4367)  loss_center_0_unscaled: 2.4374 (2.0798)  loss_ce_1_unscaled: 1.6618 (2.4425)  loss_center_1_unscaled: 2.4732 (2.1014)  loss_ce_2_unscaled: 1.6041 (2.4475)  loss_center_2_unscaled: 2.4021 (2.0875)  loss_ce_3_unscaled: 1.6742 (2.4384)  loss_center_3_unscaled: 2.3693 (2.0854)  loss_ce_4_unscaled: 1.6686 (2.4446)  loss_center_4_unscaled: 2.4299 (2.0885)  time: 0.0761  data: 0.0072  max mem: 821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  200/59143]  eta: 1:20:47  lr: 0.000100  class_error: 71.43  loss: 9.5467 (14.4825)  loss_ce: 1.5587 (2.4193)  loss_center: 0.0253 (0.0211)  loss_ce_0: 1.6073 (2.4040)  loss_ce_1: 1.5847 (2.4102)  loss_ce_2: 1.6033 (2.4141)  loss_ce_3: 1.5622 (2.4041)  loss_ce_4: 1.5578 (2.4098)  loss_ce_unscaled: 1.5587 (2.4193)  class_error_unscaled: 85.7143 (95.9544)  loss_center_unscaled: 2.5295 (2.1051)  loss_ce_0_unscaled: 1.6073 (2.4040)  loss_center_0_unscaled: 2.5733 (2.1033)  loss_ce_1_unscaled: 1.5847 (2.4102)  loss_center_1_unscaled: 2.6963 (2.1279)  loss_ce_2_unscaled: 1.6033 (2.4141)  loss_center_2_unscaled: 2.6635 (2.1159)  loss_ce_3_unscaled: 1.5622 (2.4041)  loss_center_3_unscaled: 2.5485 (2.1106)  loss_ce_4_unscaled: 1.5578 (2.4098)  loss_center_4_unscaled: 2.5575 (2.1111)  time: 0.0708  data: 0.0078  max mem: 821\n",
      "Epoch: [0]  [  210/59143]  eta: 1:20:18  lr: 0.000100  class_error: 100.00  loss: 9.2519 (14.2938)  loss_ce: 1.5587 (2.3865)  loss_center: 0.0247 (0.0212)  loss_ce_0: 1.6075 (2.3743)  loss_ce_1: 1.5215 (2.3801)  loss_ce_2: 1.5638 (2.3822)  loss_ce_3: 1.5462 (2.3718)  loss_ce_4: 1.5598 (2.3777)  loss_ce_unscaled: 1.5587 (2.3865)  class_error_unscaled: 83.3333 (95.5319)  loss_center_unscaled: 2.4668 (2.1191)  loss_ce_0_unscaled: 1.6075 (2.3743)  loss_center_0_unscaled: 2.4834 (2.1190)  loss_ce_1_unscaled: 1.5215 (2.3801)  loss_center_1_unscaled: 2.6038 (2.1439)  loss_ce_2_unscaled: 1.5638 (2.3822)  loss_center_2_unscaled: 2.4926 (2.1303)  loss_ce_3_unscaled: 1.5462 (2.3718)  loss_center_3_unscaled: 2.4996 (2.1246)  loss_ce_4_unscaled: 1.5598 (2.3777)  loss_center_4_unscaled: 2.4556 (2.1253)  time: 0.0666  data: 0.0068  max mem: 821\n",
      "Epoch: [0]  [  220/59143]  eta: 1:19:34  lr: 0.000100  class_error: 85.71  loss: 9.7253 (14.1575)  loss_ce: 1.6312 (2.3628)  loss_center: 0.0239 (0.0213)  loss_ce_0: 1.6601 (2.3540)  loss_ce_1: 1.6593 (2.3574)  loss_ce_2: 1.6685 (2.3589)  loss_ce_3: 1.6159 (2.3487)  loss_ce_4: 1.6365 (2.3543)  loss_ce_unscaled: 1.6312 (2.3628)  class_error_unscaled: 87.5000 (95.2520)  loss_center_unscaled: 2.3932 (2.1316)  loss_ce_0_unscaled: 1.6601 (2.3540)  loss_center_0_unscaled: 2.4495 (2.1346)  loss_ce_1_unscaled: 1.6593 (2.3574)  loss_center_1_unscaled: 2.4102 (2.1580)  loss_ce_2_unscaled: 1.6685 (2.3589)  loss_center_2_unscaled: 2.3903 (2.1423)  loss_ce_3_unscaled: 1.6159 (2.3487)  loss_center_3_unscaled: 2.3947 (2.1375)  loss_ce_4_unscaled: 1.6365 (2.3543)  loss_center_4_unscaled: 2.3666 (2.1386)  time: 0.0688  data: 0.0060  max mem: 821\n",
      "Epoch: [0]  [  230/59143]  eta: 1:18:58  lr: 0.000100  class_error: 100.00  loss: 8.8093 (13.8847)  loss_ce: 1.4251 (2.3156)  loss_center: 0.0243 (0.0215)  loss_ce_0: 1.5610 (2.3109)  loss_ce_1: 1.5382 (2.3122)  loss_ce_2: 1.4474 (2.3141)  loss_ce_3: 1.4105 (2.3031)  loss_ce_4: 1.4248 (2.3074)  loss_ce_unscaled: 1.4251 (2.3156)  class_error_unscaled: 85.7143 (94.5473)  loss_center_unscaled: 2.4329 (2.1465)  loss_ce_0_unscaled: 1.5610 (2.3109)  loss_center_0_unscaled: 2.3856 (2.1533)  loss_ce_1_unscaled: 1.5382 (2.3122)  loss_center_1_unscaled: 2.4016 (2.1736)  loss_ce_2_unscaled: 1.4474 (2.3141)  loss_center_2_unscaled: 2.3941 (2.1594)  loss_ce_3_unscaled: 1.4105 (2.3031)  loss_center_3_unscaled: 2.4233 (2.1541)  loss_ce_4_unscaled: 1.4248 (2.3074)  loss_center_4_unscaled: 2.4373 (2.1534)  time: 0.0663  data: 0.0057  max mem: 821\n",
      "Epoch: [0]  [  240/59143]  eta: 1:18:38  lr: 0.000100  class_error: 77.27  loss: 7.7965 (13.7742)  loss_ce: 1.2534 (2.2955)  loss_center: 0.0253 (0.0217)  loss_ce_0: 1.3566 (2.2950)  loss_ce_1: 1.3466 (2.2956)  loss_ce_2: 1.3340 (2.2960)  loss_ce_3: 1.3113 (2.2829)  loss_ce_4: 1.1791 (2.2876)  loss_ce_unscaled: 1.2534 (2.2955)  class_error_unscaled: 83.3333 (94.2035)  loss_center_unscaled: 2.5313 (2.1671)  loss_ce_0_unscaled: 1.3566 (2.2950)  loss_center_0_unscaled: 2.5968 (2.1767)  loss_ce_1_unscaled: 1.3466 (2.2956)  loss_center_1_unscaled: 2.5939 (2.1961)  loss_ce_2_unscaled: 1.3340 (2.2960)  loss_center_2_unscaled: 2.6024 (2.1799)  loss_ce_3_unscaled: 1.3113 (2.2829)  loss_center_3_unscaled: 2.5230 (2.1737)  loss_ce_4_unscaled: 1.1791 (2.2876)  loss_center_4_unscaled: 2.5595 (2.1740)  time: 0.0699  data: 0.0058  max mem: 821\n",
      "Epoch: [0]  [  250/59143]  eta: 1:18:31  lr: 0.000100  class_error: 100.00  loss: 9.7516 (13.6766)  loss_ce: 1.5723 (2.2779)  loss_center: 0.0254 (0.0219)  loss_ce_0: 1.7365 (2.2809)  loss_ce_1: 1.7390 (2.2797)  loss_ce_2: 1.6266 (2.2796)  loss_ce_3: 1.6088 (2.2661)  loss_ce_4: 1.5732 (2.2705)  loss_ce_unscaled: 1.5723 (2.2779)  class_error_unscaled: 92.5926 (94.1115)  loss_center_unscaled: 2.5399 (2.1876)  loss_ce_0_unscaled: 1.7365 (2.2809)  loss_center_0_unscaled: 2.6337 (2.1982)  loss_ce_1_unscaled: 1.7390 (2.2797)  loss_center_1_unscaled: 2.6387 (2.2172)  loss_ce_2_unscaled: 1.6266 (2.2796)  loss_center_2_unscaled: 2.6054 (2.2005)  loss_ce_3_unscaled: 1.6088 (2.2661)  loss_center_3_unscaled: 2.5550 (2.1958)  loss_ce_4_unscaled: 1.5732 (2.2705)  loss_center_4_unscaled: 2.5595 (2.1952)  time: 0.0751  data: 0.0069  max mem: 821\n",
      "Epoch: [0]  [  260/59143]  eta: 1:17:53  lr: 0.000100  class_error: 81.82  loss: 10.0804 (13.5462)  loss_ce: 1.6013 (2.2547)  loss_center: 0.0258 (0.0221)  loss_ce_0: 1.8348 (2.2618)  loss_ce_1: 1.7390 (2.2582)  loss_ce_2: 1.7115 (2.2579)  loss_ce_3: 1.6525 (2.2436)  loss_ce_4: 1.5873 (2.2479)  loss_ce_unscaled: 1.6013 (2.2547)  class_error_unscaled: 97.5610 (93.8461)  loss_center_unscaled: 2.5799 (2.2053)  loss_ce_0_unscaled: 1.8348 (2.2618)  loss_center_0_unscaled: 2.6488 (2.2166)  loss_ce_1_unscaled: 1.7390 (2.2582)  loss_center_1_unscaled: 2.6387 (2.2359)  loss_ce_2_unscaled: 1.7115 (2.2579)  loss_center_2_unscaled: 2.6054 (2.2185)  loss_ce_3_unscaled: 1.6525 (2.2436)  loss_center_3_unscaled: 2.7016 (2.2150)  loss_ce_4_unscaled: 1.5873 (2.2479)  loss_center_4_unscaled: 2.6063 (2.2136)  time: 0.0705  data: 0.0072  max mem: 821\n",
      "Epoch: [0]  [  270/59143]  eta: 1:17:14  lr: 0.000100  class_error: 48.00  loss: 9.2599 (13.3751)  loss_ce: 1.5156 (2.2256)  loss_center: 0.0267 (0.0222)  loss_ce_0: 1.7486 (2.2345)  loss_ce_1: 1.6620 (2.2299)  loss_ce_2: 1.4931 (2.2297)  loss_ce_3: 1.5085 (2.2142)  loss_ce_4: 1.4798 (2.2190)  loss_ce_unscaled: 1.5156 (2.2256)  class_error_unscaled: 81.8182 (93.1347)  loss_center_unscaled: 2.6703 (2.2215)  loss_ce_0_unscaled: 1.7486 (2.2345)  loss_center_0_unscaled: 2.5936 (2.2334)  loss_ce_1_unscaled: 1.6620 (2.2299)  loss_center_1_unscaled: 2.6302 (2.2531)  loss_ce_2_unscaled: 1.4931 (2.2297)  loss_center_2_unscaled: 2.6185 (2.2354)  loss_ce_3_unscaled: 1.5085 (2.2142)  loss_center_3_unscaled: 2.6526 (2.2303)  loss_ce_4_unscaled: 1.4798 (2.2190)  loss_center_4_unscaled: 2.6683 (2.2297)  time: 0.0625  data: 0.0068  max mem: 821\n",
      "Epoch: [0]  [  280/59143]  eta: 1:16:39  lr: 0.000100  class_error: 56.10  loss: 6.9927 (13.2372)  loss_ce: 1.1343 (2.2017)  loss_center: 0.0248 (0.0224)  loss_ce_0: 1.2133 (2.2133)  loss_ce_1: 1.1747 (2.2077)  loss_ce_2: 1.1605 (2.2067)  loss_ce_3: 1.1392 (2.1905)  loss_ce_4: 1.1548 (2.1950)  loss_ce_unscaled: 1.1343 (2.2017)  class_error_unscaled: 75.0000 (92.5576)  loss_center_unscaled: 2.4847 (2.2364)  loss_ce_0_unscaled: 1.2133 (2.2133)  loss_center_0_unscaled: 2.5825 (2.2481)  loss_ce_1_unscaled: 1.1747 (2.2077)  loss_center_1_unscaled: 2.6249 (2.2684)  loss_ce_2_unscaled: 1.1605 (2.2067)  loss_center_2_unscaled: 2.4880 (2.2483)  loss_ce_3_unscaled: 1.1392 (2.1905)  loss_center_3_unscaled: 2.4729 (2.2423)  loss_ce_4_unscaled: 1.1548 (2.1950)  loss_center_4_unscaled: 2.4781 (2.2414)  time: 0.0620  data: 0.0064  max mem: 821\n",
      "Epoch: [0]  [  290/59143]  eta: 1:16:11  lr: 0.000100  class_error: 71.43  loss: 10.2037 (13.1232)  loss_ce: 1.6722 (2.1821)  loss_center: 0.0260 (0.0225)  loss_ce_0: 1.8403 (2.1968)  loss_ce_1: 1.6452 (2.1879)  loss_ce_2: 1.6516 (2.1878)  loss_ce_3: 1.6552 (2.1706)  loss_ce_4: 1.6703 (2.1753)  loss_ce_unscaled: 1.6722 (2.1821)  class_error_unscaled: 71.4286 (92.0330)  loss_center_unscaled: 2.5990 (2.2522)  loss_ce_0_unscaled: 1.8403 (2.1968)  loss_center_0_unscaled: 2.5956 (2.2633)  loss_ce_1_unscaled: 1.6452 (2.1879)  loss_center_1_unscaled: 2.6497 (2.2834)  loss_ce_2_unscaled: 1.6516 (2.1878)  loss_center_2_unscaled: 2.6347 (2.2645)  loss_ce_3_unscaled: 1.6552 (2.1706)  loss_center_3_unscaled: 2.5766 (2.2576)  loss_ce_4_unscaled: 1.6703 (2.1753)  loss_center_4_unscaled: 2.5550 (2.2568)  time: 0.0636  data: 0.0067  max mem: 821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  300/59143]  eta: 1:15:55  lr: 0.000100  class_error: 75.00  loss: 8.5842 (12.9407)  loss_ce: 1.4194 (2.1510)  loss_center: 0.0267 (0.0227)  loss_ce_0: 1.4576 (2.1680)  loss_ce_1: 1.4790 (2.1583)  loss_ce_2: 1.4619 (2.1571)  loss_ce_3: 1.3582 (2.1394)  loss_ce_4: 1.4245 (2.1443)  loss_ce_unscaled: 1.4194 (2.1510)  class_error_unscaled: 65.2174 (90.7744)  loss_center_unscaled: 2.6673 (2.2673)  loss_ce_0_unscaled: 1.4576 (2.1680)  loss_center_0_unscaled: 2.6309 (2.2764)  loss_ce_1_unscaled: 1.4790 (2.1583)  loss_center_1_unscaled: 2.7253 (2.2983)  loss_ce_2_unscaled: 1.4619 (2.1571)  loss_center_2_unscaled: 2.6658 (2.2784)  loss_ce_3_unscaled: 1.3582 (2.1394)  loss_center_3_unscaled: 2.6153 (2.2716)  loss_ce_4_unscaled: 1.4245 (2.1443)  loss_center_4_unscaled: 2.6421 (2.2718)  time: 0.0672  data: 0.0074  max mem: 821\n",
      "Epoch: [0]  [  310/59143]  eta: 1:15:36  lr: 0.000100  class_error: 75.00  loss: 8.4059 (12.8486)  loss_ce: 1.3529 (2.1349)  loss_center: 0.0271 (0.0228)  loss_ce_0: 1.4106 (2.1550)  loss_ce_1: 1.4553 (2.1434)  loss_ce_2: 1.3816 (2.1413)  loss_ce_3: 1.2762 (2.1234)  loss_ce_4: 1.3011 (2.1277)  loss_ce_unscaled: 1.3529 (2.1349)  class_error_unscaled: 71.4286 (90.5361)  loss_center_unscaled: 2.7069 (2.2848)  loss_ce_0_unscaled: 1.4106 (2.1550)  loss_center_0_unscaled: 2.7137 (2.2955)  loss_ce_1_unscaled: 1.4553 (2.1434)  loss_center_1_unscaled: 2.7792 (2.3164)  loss_ce_2_unscaled: 1.3816 (2.1413)  loss_center_2_unscaled: 2.6658 (2.2957)  loss_ce_3_unscaled: 1.2762 (2.1234)  loss_center_3_unscaled: 2.6881 (2.2887)  loss_ce_4_unscaled: 1.3011 (2.1277)  loss_center_4_unscaled: 2.6958 (2.2890)  time: 0.0689  data: 0.0072  max mem: 821\n",
      "Epoch: [0]  [  320/59143]  eta: 1:15:11  lr: 0.000100  class_error: 65.79  loss: 7.6737 (12.7569)  loss_ce: 1.2426 (2.1193)  loss_center: 0.0271 (0.0230)  loss_ce_0: 1.2757 (2.1410)  loss_ce_1: 1.3002 (2.1288)  loss_ce_2: 1.2776 (2.1254)  loss_ce_3: 1.2342 (2.1077)  loss_ce_4: 1.2082 (2.1117)  loss_ce_unscaled: 1.2426 (2.1193)  class_error_unscaled: 75.0000 (89.5777)  loss_center_unscaled: 2.7069 (2.2985)  loss_ce_0_unscaled: 1.2757 (2.1410)  loss_center_0_unscaled: 2.7261 (2.3069)  loss_ce_1_unscaled: 1.3002 (2.1288)  loss_center_1_unscaled: 2.7302 (2.3282)  loss_ce_2_unscaled: 1.2776 (2.1254)  loss_center_2_unscaled: 2.7215 (2.3071)  loss_ce_3_unscaled: 1.2342 (2.1077)  loss_center_3_unscaled: 2.6936 (2.3015)  loss_ce_4_unscaled: 1.2082 (2.1117)  loss_center_4_unscaled: 2.6838 (2.3015)  time: 0.0660  data: 0.0067  max mem: 821\n",
      "Epoch: [0]  [  330/59143]  eta: 1:14:47  lr: 0.000100  class_error: 100.00  loss: 9.0149 (12.6727)  loss_ce: 1.4741 (2.1045)  loss_center: 0.0273 (0.0232)  loss_ce_0: 1.5490 (2.1285)  loss_ce_1: 1.4818 (2.1152)  loss_ce_2: 1.4928 (2.1112)  loss_ce_3: 1.5091 (2.0935)  loss_ce_4: 1.4550 (2.0967)  loss_ce_unscaled: 1.4741 (2.1045)  class_error_unscaled: 63.1579 (89.1717)  loss_center_unscaled: 2.7289 (2.3162)  loss_ce_0_unscaled: 1.5490 (2.1285)  loss_center_0_unscaled: 2.7604 (2.3239)  loss_ce_1_unscaled: 1.4818 (2.1152)  loss_center_1_unscaled: 2.7302 (2.3449)  loss_ce_2_unscaled: 1.4928 (2.1112)  loss_center_2_unscaled: 2.7215 (2.3242)  loss_ce_3_unscaled: 1.5091 (2.0935)  loss_center_3_unscaled: 2.7352 (2.3197)  loss_ce_4_unscaled: 1.4550 (2.0967)  loss_center_4_unscaled: 2.7023 (2.3185)  time: 0.0638  data: 0.0068  max mem: 821\n",
      "Epoch: [0]  [  340/59143]  eta: 1:14:21  lr: 0.000100  class_error: 100.00  loss: 8.9717 (12.5446)  loss_ce: 1.4741 (2.0824)  loss_center: 0.0295 (0.0234)  loss_ce_0: 1.5103 (2.1072)  loss_ce_1: 1.4818 (2.0948)  loss_ce_2: 1.4591 (2.0895)  loss_ce_3: 1.5017 (2.0721)  loss_ce_4: 1.4550 (2.0752)  loss_ce_unscaled: 1.4741 (2.0824)  class_error_unscaled: 83.3333 (88.8804)  loss_center_unscaled: 2.9524 (2.3371)  loss_ce_0_unscaled: 1.5103 (2.1072)  loss_center_0_unscaled: 2.8939 (2.3445)  loss_ce_1_unscaled: 1.4818 (2.0948)  loss_center_1_unscaled: 2.9257 (2.3682)  loss_ce_2_unscaled: 1.4591 (2.0895)  loss_center_2_unscaled: 2.8959 (2.3447)  loss_ce_3_unscaled: 1.5017 (2.0721)  loss_center_3_unscaled: 2.9626 (2.3414)  loss_ce_4_unscaled: 1.4550 (2.0752)  loss_center_4_unscaled: 2.9691 (2.3400)  time: 0.0626  data: 0.0072  max mem: 821\n",
      "Epoch: [0]  [  350/59143]  eta: 1:13:57  lr: 0.000100  class_error: 100.00  loss: 8.0982 (12.4661)  loss_ce: 1.2898 (2.0682)  loss_center: 0.0294 (0.0235)  loss_ce_0: 1.4181 (2.0952)  loss_ce_1: 1.4137 (2.0822)  loss_ce_2: 1.3149 (2.0765)  loss_ce_3: 1.2893 (2.0587)  loss_ce_4: 1.2911 (2.0618)  loss_ce_unscaled: 1.2898 (2.0682)  class_error_unscaled: 88.4615 (88.8334)  loss_center_unscaled: 2.9436 (2.3518)  loss_ce_0_unscaled: 1.4181 (2.0952)  loss_center_0_unscaled: 2.8821 (2.3603)  loss_ce_1_unscaled: 1.4137 (2.0822)  loss_center_1_unscaled: 3.0277 (2.3852)  loss_ce_2_unscaled: 1.3149 (2.0765)  loss_center_2_unscaled: 2.9024 (2.3613)  loss_ce_3_unscaled: 1.2893 (2.0587)  loss_center_3_unscaled: 2.9175 (2.3574)  loss_ce_4_unscaled: 1.2911 (2.0618)  loss_center_4_unscaled: 2.9691 (2.3560)  time: 0.0618  data: 0.0070  max mem: 821\n",
      "Epoch: [0]  [  360/59143]  eta: 1:13:50  lr: 0.000100  class_error: 71.43  loss: 9.0500 (12.4012)  loss_ce: 1.4719 (2.0568)  loss_center: 0.0283 (0.0237)  loss_ce_0: 1.5734 (2.0854)  loss_ce_1: 1.4995 (2.0711)  loss_ce_2: 1.5095 (2.0658)  loss_ce_3: 1.4617 (2.0476)  loss_ce_4: 1.5073 (2.0508)  loss_ce_unscaled: 1.4719 (2.0568)  class_error_unscaled: 80.0000 (88.4943)  loss_center_unscaled: 2.8257 (2.3668)  loss_ce_0_unscaled: 1.5734 (2.0854)  loss_center_0_unscaled: 2.8537 (2.3752)  loss_ce_1_unscaled: 1.4995 (2.0711)  loss_center_1_unscaled: 2.9017 (2.3998)  loss_ce_2_unscaled: 1.5095 (2.0658)  loss_center_2_unscaled: 2.7976 (2.3768)  loss_ce_3_unscaled: 1.4617 (2.0476)  loss_center_3_unscaled: 2.8945 (2.3734)  loss_ce_4_unscaled: 1.5073 (2.0508)  loss_center_4_unscaled: 2.9014 (2.3722)  time: 0.0668  data: 0.0071  max mem: 821\n",
      "Epoch: [0]  [  370/59143]  eta: 1:13:27  lr: 0.000100  class_error: 80.00  loss: 9.0500 (12.3268)  loss_ce: 1.4773 (2.0437)  loss_center: 0.0282 (0.0238)  loss_ce_0: 1.6108 (2.0747)  loss_ce_1: 1.4995 (2.0593)  loss_ce_2: 1.5095 (2.0530)  loss_ce_3: 1.4617 (2.0347)  loss_ce_4: 1.5095 (2.0377)  loss_ce_unscaled: 1.4773 (2.0437)  class_error_unscaled: 75.0000 (88.1155)  loss_center_unscaled: 2.8174 (2.3766)  loss_ce_0_unscaled: 1.6108 (2.0747)  loss_center_0_unscaled: 2.8592 (2.3859)  loss_ce_1_unscaled: 1.4995 (2.0593)  loss_center_1_unscaled: 2.8402 (2.4098)  loss_ce_2_unscaled: 1.5095 (2.0530)  loss_center_2_unscaled: 2.7602 (2.3867)  loss_ce_3_unscaled: 1.4617 (2.0347)  loss_center_3_unscaled: 2.8159 (2.3826)  loss_ce_4_unscaled: 1.5095 (2.0377)  loss_center_4_unscaled: 2.8488 (2.3816)  time: 0.0665  data: 0.0073  max mem: 821\n",
      "Epoch: [0]  [  380/59143]  eta: 1:13:00  lr: 0.000100  class_error: 64.71  loss: 9.7755 (12.2655)  loss_ce: 1.5811 (2.0331)  loss_center: 0.0281 (0.0239)  loss_ce_0: 1.6858 (2.0656)  loss_ce_1: 1.6729 (2.0494)  loss_ce_2: 1.6162 (2.0424)  loss_ce_3: 1.5883 (2.0239)  loss_ce_4: 1.5833 (2.0272)  loss_ce_unscaled: 1.5811 (2.0331)  class_error_unscaled: 73.3333 (87.6424)  loss_center_unscaled: 2.8148 (2.3878)  loss_ce_0_unscaled: 1.6858 (2.0656)  loss_center_0_unscaled: 2.7075 (2.3961)  loss_ce_1_unscaled: 1.6729 (2.0494)  loss_center_1_unscaled: 2.7557 (2.4207)  loss_ce_2_unscaled: 1.6162 (2.0424)  loss_center_2_unscaled: 2.7375 (2.3971)  loss_ce_3_unscaled: 1.5883 (2.0239)  loss_center_3_unscaled: 2.7860 (2.3936)  loss_ce_4_unscaled: 1.5833 (2.0272)  loss_center_4_unscaled: 2.8000 (2.3926)  time: 0.0597  data: 0.0065  max mem: 850\n",
      "Epoch: [0]  [  390/59143]  eta: 1:12:39  lr: 0.000100  class_error: 100.00  loss: 9.7755 (12.1765)  loss_ce: 1.5668 (2.0177)  loss_center: 0.0282 (0.0240)  loss_ce_0: 1.6796 (2.0520)  loss_ce_1: 1.6729 (2.0350)  loss_ce_2: 1.6057 (2.0275)  loss_ce_3: 1.5883 (2.0088)  loss_ce_4: 1.5340 (2.0116)  loss_ce_unscaled: 1.5668 (2.0177)  class_error_unscaled: 73.3333 (87.4286)  loss_center_unscaled: 2.8210 (2.4000)  loss_ce_0_unscaled: 1.6796 (2.0520)  loss_center_0_unscaled: 2.7916 (2.4072)  loss_ce_1_unscaled: 1.6729 (2.0350)  loss_center_1_unscaled: 2.9190 (2.4343)  loss_ce_2_unscaled: 1.6057 (2.0275)  loss_center_2_unscaled: 2.8409 (2.4106)  loss_ce_3_unscaled: 1.5883 (2.0088)  loss_center_3_unscaled: 2.8096 (2.4069)  loss_ce_4_unscaled: 1.5340 (2.0116)  loss_center_4_unscaled: 2.8154 (2.4047)  time: 0.0597  data: 0.0074  max mem: 850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  400/59143]  eta: 1:12:33  lr: 0.000100  class_error: 61.54  loss: 9.8115 (12.1497)  loss_ce: 1.5798 (2.0125)  loss_center: 0.0275 (0.0241)  loss_ce_0: 1.7981 (2.0486)  loss_ce_1: 1.7098 (2.0314)  loss_ce_2: 1.6325 (2.0229)  loss_ce_3: 1.6007 (2.0037)  loss_ce_4: 1.5934 (2.0064)  loss_ce_unscaled: 1.5798 (2.0125)  class_error_unscaled: 85.7143 (87.2321)  loss_center_unscaled: 2.7533 (2.4083)  loss_ce_0_unscaled: 1.7981 (2.0486)  loss_center_0_unscaled: 2.7916 (2.4172)  loss_ce_1_unscaled: 1.7098 (2.0314)  loss_center_1_unscaled: 2.8801 (2.4449)  loss_ce_2_unscaled: 1.6325 (2.0229)  loss_center_2_unscaled: 2.7775 (2.4202)  loss_ce_3_unscaled: 1.6007 (2.0037)  loss_center_3_unscaled: 2.7577 (2.4157)  loss_ce_4_unscaled: 1.5934 (2.0064)  loss_center_4_unscaled: 2.7484 (2.4132)  time: 0.0658  data: 0.0086  max mem: 850\n",
      "Epoch: [0]  [  410/59143]  eta: 1:12:15  lr: 0.000100  class_error: 41.67  loss: 9.4104 (12.0419)  loss_ce: 1.5397 (1.9942)  loss_center: 0.0278 (0.0242)  loss_ce_0: 1.5225 (2.0313)  loss_ce_1: 1.5136 (2.0128)  loss_ce_2: 1.5918 (2.0051)  loss_ce_3: 1.5661 (1.9859)  loss_ce_4: 1.5288 (1.9883)  loss_ce_unscaled: 1.5397 (1.9942)  class_error_unscaled: 85.7143 (86.7560)  loss_center_unscaled: 2.7778 (2.4204)  loss_ce_0_unscaled: 1.5225 (2.0313)  loss_center_0_unscaled: 2.8027 (2.4292)  loss_ce_1_unscaled: 1.5136 (2.0128)  loss_center_1_unscaled: 2.7836 (2.4554)  loss_ce_2_unscaled: 1.5918 (2.0051)  loss_center_2_unscaled: 2.7461 (2.4328)  loss_ce_3_unscaled: 1.5661 (1.9859)  loss_center_3_unscaled: 2.7577 (2.4279)  loss_ce_4_unscaled: 1.5288 (1.9883)  loss_center_4_unscaled: 2.7591 (2.4246)  time: 0.0662  data: 0.0071  max mem: 850\n",
      "Epoch: [0]  [  420/59143]  eta: 1:11:57  lr: 0.000100  class_error: 88.89  loss: 7.8632 (12.0156)  loss_ce: 1.2656 (1.9890)  loss_center: 0.0283 (0.0243)  loss_ce_0: 1.2960 (2.0282)  loss_ce_1: 1.2613 (2.0095)  loss_ce_2: 1.3129 (2.0010)  loss_ce_3: 1.2179 (1.9804)  loss_ce_4: 1.2677 (1.9832)  loss_ce_unscaled: 1.2656 (1.9890)  class_error_unscaled: 66.6667 (86.4638)  loss_center_unscaled: 2.8290 (2.4295)  loss_ce_0_unscaled: 1.2960 (2.0282)  loss_center_0_unscaled: 2.8224 (2.4383)  loss_ce_1_unscaled: 1.2613 (2.0095)  loss_center_1_unscaled: 2.7836 (2.4645)  loss_ce_2_unscaled: 1.3129 (2.0010)  loss_center_2_unscaled: 2.7692 (2.4419)  loss_ce_3_unscaled: 1.2179 (1.9804)  loss_center_3_unscaled: 2.7785 (2.4371)  loss_ce_4_unscaled: 1.2677 (1.9832)  loss_center_4_unscaled: 2.8046 (2.4338)  time: 0.0619  data: 0.0065  max mem: 850\n",
      "Epoch: [0]  [  430/59143]  eta: 1:11:51  lr: 0.000100  class_error: 83.33  loss: 9.8706 (11.9547)  loss_ce: 1.6201 (1.9781)  loss_center: 0.0283 (0.0244)  loss_ce_0: 1.6784 (2.0193)  loss_ce_1: 1.6552 (1.9998)  loss_ce_2: 1.6157 (1.9908)  loss_ce_3: 1.5644 (1.9700)  loss_ce_4: 1.5688 (1.9722)  loss_ce_unscaled: 1.6201 (1.9781)  class_error_unscaled: 75.0000 (86.1159)  loss_center_unscaled: 2.8290 (2.4416)  loss_ce_0_unscaled: 1.6784 (2.0193)  loss_center_0_unscaled: 2.8375 (2.4497)  loss_ce_1_unscaled: 1.6552 (1.9998)  loss_center_1_unscaled: 2.9078 (2.4754)  loss_ce_2_unscaled: 1.6157 (1.9908)  loss_center_2_unscaled: 2.8534 (2.4534)  loss_ce_3_unscaled: 1.5644 (1.9700)  loss_center_3_unscaled: 2.8791 (2.4495)  loss_ce_4_unscaled: 1.5688 (1.9722)  loss_center_4_unscaled: 2.7949 (2.4454)  time: 0.0657  data: 0.0065  max mem: 850\n",
      "Epoch: [0]  [  440/59143]  eta: 1:11:41  lr: 0.000100  class_error: 71.43  loss: 10.2896 (11.9221)  loss_ce: 1.7082 (1.9721)  loss_center: 0.0295 (0.0245)  loss_ce_0: 1.7230 (2.0145)  loss_ce_1: 1.7484 (1.9950)  loss_ce_2: 1.6568 (1.9851)  loss_ce_3: 1.6757 (1.9646)  loss_ce_4: 1.6708 (1.9663)  loss_ce_unscaled: 1.7082 (1.9721)  class_error_unscaled: 83.3333 (86.1136)  loss_center_unscaled: 2.9537 (2.4533)  loss_ce_0_unscaled: 1.7230 (2.0145)  loss_center_0_unscaled: 2.8833 (2.4592)  loss_ce_1_unscaled: 1.7484 (1.9950)  loss_center_1_unscaled: 2.9705 (2.4876)  loss_ce_2_unscaled: 1.6568 (1.9851)  loss_center_2_unscaled: 2.9936 (2.4652)  loss_ce_3_unscaled: 1.6757 (1.9646)  loss_center_3_unscaled: 2.9906 (2.4620)  loss_ce_4_unscaled: 1.6708 (1.9663)  loss_center_4_unscaled: 2.9753 (2.4572)  time: 0.0679  data: 0.0064  max mem: 850\n",
      "Epoch: [0]  [  450/59143]  eta: 1:11:28  lr: 0.000100  class_error: 53.12  loss: 10.5528 (11.9211)  loss_ce: 1.7717 (1.9714)  loss_center: 0.0283 (0.0246)  loss_ce_0: 1.8298 (2.0153)  loss_ce_1: 1.8262 (1.9954)  loss_ce_2: 1.7625 (1.9848)  loss_ce_3: 1.7423 (1.9641)  loss_ce_4: 1.7571 (1.9655)  loss_ce_unscaled: 1.7717 (1.9714)  class_error_unscaled: 83.3333 (85.9368)  loss_center_unscaled: 2.8338 (2.4602)  loss_ce_0_unscaled: 1.8298 (2.0153)  loss_center_0_unscaled: 2.8312 (2.4682)  loss_ce_1_unscaled: 1.8262 (1.9954)  loss_center_1_unscaled: 2.9397 (2.4958)  loss_ce_2_unscaled: 1.7625 (1.9848)  loss_center_2_unscaled: 2.8994 (2.4729)  loss_ce_3_unscaled: 1.7423 (1.9641)  loss_center_3_unscaled: 2.8338 (2.4694)  loss_ce_4_unscaled: 1.7571 (1.9655)  loss_center_4_unscaled: 2.8052 (2.4641)  time: 0.0652  data: 0.0069  max mem: 850\n",
      "Epoch: [0]  [  460/59143]  eta: 1:11:17  lr: 0.000100  class_error: 81.25  loss: 10.0342 (11.8485)  loss_ce: 1.6303 (1.9592)  loss_center: 0.0282 (0.0247)  loss_ce_0: 1.8040 (2.0033)  loss_ce_1: 1.6535 (1.9838)  loss_ce_2: 1.6176 (1.9723)  loss_ce_3: 1.6258 (1.9518)  loss_ce_4: 1.6714 (1.9532)  loss_ce_unscaled: 1.6303 (1.9592)  class_error_unscaled: 78.2609 (85.6864)  loss_center_unscaled: 2.8176 (2.4709)  loss_ce_0_unscaled: 1.8040 (2.0033)  loss_center_0_unscaled: 2.8487 (2.4793)  loss_ce_1_unscaled: 1.6535 (1.9838)  loss_center_1_unscaled: 2.8501 (2.5075)  loss_ce_2_unscaled: 1.6176 (1.9723)  loss_center_2_unscaled: 2.8311 (2.4841)  loss_ce_3_unscaled: 1.6258 (1.9518)  loss_center_3_unscaled: 2.8165 (2.4802)  loss_ce_4_unscaled: 1.6714 (1.9532)  loss_center_4_unscaled: 2.7994 (2.4748)  time: 0.0645  data: 0.0072  max mem: 850\n",
      "Epoch: [0]  [  470/59143]  eta: 1:11:10  lr: 0.000100  class_error: 100.00  loss: 8.8938 (11.8102)  loss_ce: 1.5240 (1.9521)  loss_center: 0.0301 (0.0248)  loss_ce_0: 1.4429 (1.9971)  loss_ce_1: 1.5026 (1.9781)  loss_ce_2: 1.5299 (1.9661)  loss_ce_3: 1.5789 (1.9455)  loss_ce_4: 1.5333 (1.9465)  loss_ce_unscaled: 1.5240 (1.9521)  class_error_unscaled: 75.0000 (85.4337)  loss_center_unscaled: 3.0077 (2.4823)  loss_ce_0_unscaled: 1.4429 (1.9971)  loss_center_0_unscaled: 3.0073 (2.4908)  loss_ce_1_unscaled: 1.5026 (1.9781)  loss_center_1_unscaled: 3.0592 (2.5198)  loss_ce_2_unscaled: 1.5299 (1.9661)  loss_center_2_unscaled: 2.9864 (2.4955)  loss_ce_3_unscaled: 1.5789 (1.9455)  loss_center_3_unscaled: 2.9823 (2.4923)  loss_ce_4_unscaled: 1.5333 (1.9465)  loss_center_4_unscaled: 3.0025 (2.4865)  time: 0.0662  data: 0.0066  max mem: 850\n",
      "Epoch: [0]  [  480/59143]  eta: 1:10:52  lr: 0.000100  class_error: 100.00  loss: 8.2164 (11.7397)  loss_ce: 1.3422 (1.9402)  loss_center: 0.0303 (0.0249)  loss_ce_0: 1.4117 (1.9861)  loss_ce_1: 1.3923 (1.9667)  loss_ce_2: 1.4215 (1.9542)  loss_ce_3: 1.3034 (1.9334)  loss_ce_4: 1.3197 (1.9341)  loss_ce_unscaled: 1.3422 (1.9402)  class_error_unscaled: 75.0000 (85.1863)  loss_center_unscaled: 3.0253 (2.4946)  loss_ce_0_unscaled: 1.4117 (1.9861)  loss_center_0_unscaled: 3.0074 (2.5014)  loss_ce_1_unscaled: 1.3923 (1.9667)  loss_center_1_unscaled: 3.0167 (2.5312)  loss_ce_2_unscaled: 1.4215 (1.9542)  loss_center_2_unscaled: 2.9970 (2.5072)  loss_ce_3_unscaled: 1.3034 (1.9334)  loss_center_3_unscaled: 3.0011 (2.5042)  loss_ce_4_unscaled: 1.3197 (1.9341)  loss_center_4_unscaled: 3.0270 (2.4979)  time: 0.0632  data: 0.0059  max mem: 850\n",
      "Epoch: [0]  [  490/59143]  eta: 1:10:55  lr: 0.000100  class_error: 66.67  loss: 7.5690 (11.6637)  loss_ce: 1.1795 (1.9274)  loss_center: 0.0296 (0.0250)  loss_ce_0: 1.3116 (1.9742)  loss_ce_1: 1.2909 (1.9541)  loss_ce_2: 1.3171 (1.9416)  loss_ce_3: 1.2052 (1.9203)  loss_ce_4: 1.1792 (1.9210)  loss_ce_unscaled: 1.1795 (1.9274)  class_error_unscaled: 75.0000 (84.9265)  loss_center_unscaled: 2.9580 (2.5030)  loss_ce_0_unscaled: 1.3116 (1.9742)  loss_center_0_unscaled: 2.9042 (2.5091)  loss_ce_1_unscaled: 1.2909 (1.9541)  loss_center_1_unscaled: 2.9219 (2.5388)  loss_ce_2_unscaled: 1.3171 (1.9416)  loss_center_2_unscaled: 2.9098 (2.5146)  loss_ce_3_unscaled: 1.2052 (1.9203)  loss_center_3_unscaled: 2.9825 (2.5120)  loss_ce_4_unscaled: 1.1792 (1.9210)  loss_center_4_unscaled: 2.9272 (2.5055)  time: 0.0671  data: 0.0066  max mem: 850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  500/59143]  eta: 1:10:46  lr: 0.000100  class_error: 41.67  loss: 9.2511 (11.6445)  loss_ce: 1.5088 (1.9240)  loss_center: 0.0303 (0.0252)  loss_ce_0: 1.5968 (1.9712)  loss_ce_1: 1.5652 (1.9508)  loss_ce_2: 1.5767 (1.9383)  loss_ce_3: 1.5088 (1.9175)  loss_ce_4: 1.5018 (1.9175)  loss_ce_unscaled: 1.5088 (1.9240)  class_error_unscaled: 76.9231 (84.7728)  loss_center_unscaled: 3.0307 (2.5158)  loss_ce_0_unscaled: 1.5968 (1.9712)  loss_center_0_unscaled: 2.9518 (2.5209)  loss_ce_1_unscaled: 1.5652 (1.9508)  loss_center_1_unscaled: 2.9902 (2.5506)  loss_ce_2_unscaled: 1.5767 (1.9383)  loss_center_2_unscaled: 2.9582 (2.5269)  loss_ce_3_unscaled: 1.5088 (1.9175)  loss_center_3_unscaled: 2.9984 (2.5251)  loss_ce_4_unscaled: 1.5018 (1.9175)  loss_center_4_unscaled: 3.0079 (2.5180)  time: 0.0704  data: 0.0063  max mem: 850\n",
      "Epoch: [0]  [  510/59143]  eta: 1:10:43  lr: 0.000100  class_error: 33.33  loss: 7.7999 (11.5582)  loss_ce: 1.2779 (1.9098)  loss_center: 0.0312 (0.0253)  loss_ce_0: 1.3542 (1.9568)  loss_ce_1: 1.2670 (1.9365)  loss_ce_2: 1.2875 (1.9238)  loss_ce_3: 1.2896 (1.9030)  loss_ce_4: 1.3091 (1.9030)  loss_ce_unscaled: 1.2779 (1.9098)  class_error_unscaled: 75.0000 (84.5301)  loss_center_unscaled: 3.1150 (2.5280)  loss_ce_0_unscaled: 1.3542 (1.9568)  loss_center_0_unscaled: 3.0375 (2.5324)  loss_ce_1_unscaled: 1.2670 (1.9365)  loss_center_1_unscaled: 3.0921 (2.5624)  loss_ce_2_unscaled: 1.2875 (1.9238)  loss_center_2_unscaled: 3.0845 (2.5384)  loss_ce_3_unscaled: 1.2896 (1.9030)  loss_center_3_unscaled: 3.0595 (2.5363)  loss_ce_4_unscaled: 1.3091 (1.9030)  loss_center_4_unscaled: 3.0563 (2.5291)  time: 0.0679  data: 0.0058  max mem: 850\n",
      "Epoch: [0]  [  520/59143]  eta: 1:10:32  lr: 0.000100  class_error: 100.00  loss: 7.6770 (11.5016)  loss_ce: 1.2526 (1.9002)  loss_center: 0.0309 (0.0254)  loss_ce_0: 1.3120 (1.9483)  loss_ce_1: 1.2519 (1.9266)  loss_ce_2: 1.2374 (1.9141)  loss_ce_3: 1.2339 (1.8936)  loss_ce_4: 1.2718 (1.8934)  loss_ce_unscaled: 1.2526 (1.9002)  class_error_unscaled: 66.6667 (84.1012)  loss_center_unscaled: 3.0938 (2.5394)  loss_ce_0_unscaled: 1.3120 (1.9483)  loss_center_0_unscaled: 3.0937 (2.5435)  loss_ce_1_unscaled: 1.2519 (1.9266)  loss_center_1_unscaled: 3.0639 (2.5729)  loss_ce_2_unscaled: 1.2374 (1.9141)  loss_center_2_unscaled: 3.0845 (2.5494)  loss_ce_3_unscaled: 1.2339 (1.8936)  loss_center_3_unscaled: 3.1030 (2.5475)  loss_ce_4_unscaled: 1.2718 (1.8934)  loss_center_4_unscaled: 3.0701 (2.5407)  time: 0.0668  data: 0.0056  max mem: 850\n",
      "Epoch: [0]  [  530/59143]  eta: 1:10:26  lr: 0.000100  class_error: 100.00  loss: 8.1835 (11.4359)  loss_ce: 1.3199 (1.8891)  loss_center: 0.0314 (0.0255)  loss_ce_0: 1.4373 (1.9378)  loss_ce_1: 1.3707 (1.9152)  loss_ce_2: 1.3882 (1.9030)  loss_ce_3: 1.3143 (1.8828)  loss_ce_4: 1.3212 (1.8825)  loss_ce_unscaled: 1.3199 (1.8891)  class_error_unscaled: 66.6667 (84.0137)  loss_center_unscaled: 3.1372 (2.5518)  loss_ce_0_unscaled: 1.4373 (1.9378)  loss_center_0_unscaled: 3.1146 (2.5553)  loss_ce_1_unscaled: 1.3707 (1.9152)  loss_center_1_unscaled: 3.0960 (2.5838)  loss_ce_2_unscaled: 1.3882 (1.9030)  loss_center_2_unscaled: 3.1133 (2.5613)  loss_ce_3_unscaled: 1.3143 (1.8828)  loss_center_3_unscaled: 3.1166 (2.5595)  loss_ce_4_unscaled: 1.3212 (1.8825)  loss_center_4_unscaled: 3.1346 (2.5532)  time: 0.0657  data: 0.0053  max mem: 850\n",
      "Epoch: [0]  [  540/59143]  eta: 1:10:13  lr: 0.000100  class_error: 100.00  loss: 9.5408 (11.4141)  loss_ce: 1.5597 (1.8852)  loss_center: 0.0317 (0.0256)  loss_ce_0: 1.6177 (1.9354)  loss_ce_1: 1.6364 (1.9117)  loss_ce_2: 1.5448 (1.8991)  loss_ce_3: 1.5688 (1.8788)  loss_ce_4: 1.5256 (1.8783)  loss_ce_unscaled: 1.5597 (1.8852)  class_error_unscaled: 77.7778 (83.9132)  loss_center_unscaled: 3.1663 (2.5615)  loss_ce_0_unscaled: 1.6177 (1.9354)  loss_center_0_unscaled: 3.1647 (2.5651)  loss_ce_1_unscaled: 1.6364 (1.9117)  loss_center_1_unscaled: 3.1398 (2.5940)  loss_ce_2_unscaled: 1.5448 (1.8991)  loss_center_2_unscaled: 3.1916 (2.5720)  loss_ce_3_unscaled: 1.5688 (1.8788)  loss_center_3_unscaled: 3.2052 (2.5700)  loss_ce_4_unscaled: 1.5256 (1.8783)  loss_center_4_unscaled: 3.1916 (2.5628)  time: 0.0643  data: 0.0067  max mem: 850\n",
      "Epoch: [0]  [  550/59143]  eta: 1:10:04  lr: 0.000100  class_error: 75.00  loss: 9.7487 (11.3682)  loss_ce: 1.5802 (1.8775)  loss_center: 0.0321 (0.0258)  loss_ce_0: 1.5935 (1.9272)  loss_ce_1: 1.6859 (1.9044)  loss_ce_2: 1.5448 (1.8916)  loss_ce_3: 1.5688 (1.8711)  loss_ce_4: 1.5256 (1.8706)  loss_ce_unscaled: 1.5802 (1.8775)  class_error_unscaled: 80.0000 (83.7501)  loss_center_unscaled: 3.2061 (2.5759)  loss_ce_0_unscaled: 1.5935 (1.9272)  loss_center_0_unscaled: 3.1943 (2.5778)  loss_ce_1_unscaled: 1.6859 (1.9044)  loss_center_1_unscaled: 3.1841 (2.6064)  loss_ce_2_unscaled: 1.5448 (1.8916)  loss_center_2_unscaled: 3.2155 (2.5848)  loss_ce_3_unscaled: 1.5688 (1.8711)  loss_center_3_unscaled: 3.2615 (2.5847)  loss_ce_4_unscaled: 1.5256 (1.8706)  loss_center_4_unscaled: 3.2281 (2.5773)  time: 0.0621  data: 0.0070  max mem: 850\n",
      "Epoch: [0]  [  560/59143]  eta: 1:09:53  lr: 0.000100  class_error: 88.24  loss: 8.4208 (11.3128)  loss_ce: 1.3102 (1.8677)  loss_center: 0.0320 (0.0259)  loss_ce_0: 1.4979 (1.9192)  loss_ce_1: 1.4360 (1.8957)  loss_ce_2: 1.4403 (1.8824)  loss_ce_3: 1.3545 (1.8613)  loss_ce_4: 1.3223 (1.8606)  loss_ce_unscaled: 1.3102 (1.8677)  class_error_unscaled: 60.0000 (83.3323)  loss_center_unscaled: 3.2023 (2.5867)  loss_ce_0_unscaled: 1.4979 (1.9192)  loss_center_0_unscaled: 3.1780 (2.5887)  loss_ce_1_unscaled: 1.4360 (1.8957)  loss_center_1_unscaled: 3.2343 (2.6181)  loss_ce_2_unscaled: 1.4403 (1.8824)  loss_center_2_unscaled: 3.2416 (2.5962)  loss_ce_3_unscaled: 1.3545 (1.8613)  loss_center_3_unscaled: 3.2712 (2.5956)  loss_ce_4_unscaled: 1.3223 (1.8606)  loss_center_4_unscaled: 3.1841 (2.5880)  time: 0.0629  data: 0.0076  max mem: 850\n",
      "Epoch: [0]  [  570/59143]  eta: 1:09:44  lr: 0.000100  class_error: 66.67  loss: 8.4208 (11.2926)  loss_ce: 1.3102 (1.8640)  loss_center: 0.0309 (0.0260)  loss_ce_0: 1.4303 (1.9163)  loss_ce_1: 1.4056 (1.8923)  loss_ce_2: 1.4403 (1.8791)  loss_ce_3: 1.3545 (1.8578)  loss_ce_4: 1.3223 (1.8571)  loss_ce_unscaled: 1.3102 (1.8640)  class_error_unscaled: 66.6667 (83.1795)  loss_center_unscaled: 3.0867 (2.5977)  loss_ce_0_unscaled: 1.4303 (1.9163)  loss_center_0_unscaled: 3.1132 (2.5997)  loss_ce_1_unscaled: 1.4056 (1.8923)  loss_center_1_unscaled: 3.1901 (2.6292)  loss_ce_2_unscaled: 1.4403 (1.8791)  loss_center_2_unscaled: 3.1239 (2.6082)  loss_ce_3_unscaled: 1.3545 (1.8578)  loss_center_3_unscaled: 3.0854 (2.6068)  loss_ce_4_unscaled: 1.3223 (1.8571)  loss_center_4_unscaled: 3.1087 (2.5992)  time: 0.0628  data: 0.0088  max mem: 850\n",
      "Epoch: [0]  [  580/59143]  eta: 1:09:28  lr: 0.000100  class_error: 66.67  loss: 7.9472 (11.2553)  loss_ce: 1.2717 (1.8579)  loss_center: 0.0327 (0.0261)  loss_ce_0: 1.3413 (1.9104)  loss_ce_1: 1.3815 (1.8862)  loss_ce_2: 1.3593 (1.8722)  loss_ce_3: 1.3173 (1.8514)  loss_ce_4: 1.2558 (1.8512)  loss_ce_unscaled: 1.2717 (1.8579)  class_error_unscaled: 75.0000 (83.0171)  loss_center_unscaled: 3.2689 (2.6115)  loss_ce_0_unscaled: 1.3413 (1.9104)  loss_center_0_unscaled: 3.2372 (2.6123)  loss_ce_1_unscaled: 1.3815 (1.8862)  loss_center_1_unscaled: 3.2594 (2.6416)  loss_ce_2_unscaled: 1.3593 (1.8722)  loss_center_2_unscaled: 3.2791 (2.6205)  loss_ce_3_unscaled: 1.3173 (1.8514)  loss_center_3_unscaled: 3.2896 (2.6201)  loss_ce_4_unscaled: 1.2558 (1.8512)  loss_center_4_unscaled: 3.2690 (2.6132)  time: 0.0601  data: 0.0067  max mem: 850\n",
      "Epoch: [0]  [  590/59143]  eta: 1:09:21  lr: 0.000100  class_error: 100.00  loss: 7.7254 (11.2365)  loss_ce: 1.2363 (1.8545)  loss_center: 0.0333 (0.0262)  loss_ce_0: 1.3413 (1.9078)  loss_ce_1: 1.3733 (1.8832)  loss_ce_2: 1.2868 (1.8689)  loss_ce_3: 1.2960 (1.8481)  loss_ce_4: 1.2598 (1.8479)  loss_ce_unscaled: 1.2363 (1.8545)  class_error_unscaled: 75.0000 (82.9457)  loss_center_unscaled: 3.3345 (2.6236)  loss_ce_0_unscaled: 1.3413 (1.9078)  loss_center_0_unscaled: 3.2603 (2.6230)  loss_ce_1_unscaled: 1.3733 (1.8832)  loss_center_1_unscaled: 3.3714 (2.6535)  loss_ce_2_unscaled: 1.2868 (1.8689)  loss_center_2_unscaled: 3.3172 (2.6330)  loss_ce_3_unscaled: 1.2960 (1.8481)  loss_center_3_unscaled: 3.3544 (2.6322)  loss_ce_4_unscaled: 1.2598 (1.8479)  loss_center_4_unscaled: 3.3512 (2.6255)  time: 0.0606  data: 0.0060  max mem: 850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  600/59143]  eta: 1:09:20  lr: 0.000100  class_error: 83.33  loss: 8.4180 (11.1880)  loss_ce: 1.4166 (1.8458)  loss_center: 0.0328 (0.0263)  loss_ce_0: 1.4680 (1.9006)  loss_ce_1: 1.4080 (1.8756)  loss_ce_2: 1.3520 (1.8603)  loss_ce_3: 1.4102 (1.8395)  loss_ce_4: 1.4047 (1.8398)  loss_ce_unscaled: 1.4166 (1.8458)  class_error_unscaled: 75.0000 (82.7538)  loss_center_unscaled: 3.2775 (2.6336)  loss_ce_0_unscaled: 1.4680 (1.9006)  loss_center_0_unscaled: 3.2319 (2.6335)  loss_ce_1_unscaled: 1.4080 (1.8756)  loss_center_1_unscaled: 3.3271 (2.6639)  loss_ce_2_unscaled: 1.3520 (1.8603)  loss_center_2_unscaled: 3.3000 (2.6440)  loss_ce_3_unscaled: 1.4102 (1.8395)  loss_center_3_unscaled: 3.2747 (2.6425)  loss_ce_4_unscaled: 1.4047 (1.8398)  loss_center_4_unscaled: 3.2644 (2.6360)  time: 0.0679  data: 0.0075  max mem: 850\n",
      "Epoch: [0]  [  610/59143]  eta: 1:09:12  lr: 0.000100  class_error: 77.78  loss: 7.9058 (11.1479)  loss_ce: 1.3470 (1.8391)  loss_center: 0.0322 (0.0265)  loss_ce_0: 1.4594 (1.8940)  loss_ce_1: 1.3888 (1.8691)  loss_ce_2: 1.2914 (1.8536)  loss_ce_3: 1.3430 (1.8326)  loss_ce_4: 1.3824 (1.8331)  loss_ce_unscaled: 1.3470 (1.8391)  class_error_unscaled: 77.7778 (82.6586)  loss_center_unscaled: 3.2238 (2.6463)  loss_ce_0_unscaled: 1.4594 (1.8940)  loss_center_0_unscaled: 3.2399 (2.6460)  loss_ce_1_unscaled: 1.3888 (1.8691)  loss_center_1_unscaled: 3.3017 (2.6767)  loss_ce_2_unscaled: 1.2914 (1.8536)  loss_center_2_unscaled: 3.3026 (2.6567)  loss_ce_3_unscaled: 1.3430 (1.8326)  loss_center_3_unscaled: 3.2678 (2.6548)  loss_ce_4_unscaled: 1.3824 (1.8331)  loss_center_4_unscaled: 3.2868 (2.6487)  time: 0.0671  data: 0.0077  max mem: 850\n",
      "Epoch: [0]  [  620/59143]  eta: 1:09:07  lr: 0.000100  class_error: 90.91  loss: 8.6878 (11.1013)  loss_ce: 1.4059 (1.8308)  loss_center: 0.0325 (0.0266)  loss_ce_0: 1.5007 (1.8867)  loss_ce_1: 1.4478 (1.8616)  loss_ce_2: 1.4146 (1.8459)  loss_ce_3: 1.3577 (1.8246)  loss_ce_4: 1.4226 (1.8252)  loss_ce_unscaled: 1.4059 (1.8308)  class_error_unscaled: 75.0000 (82.4325)  loss_center_unscaled: 3.2511 (2.6552)  loss_ce_0_unscaled: 1.5007 (1.8867)  loss_center_0_unscaled: 3.2785 (2.6558)  loss_ce_1_unscaled: 1.4478 (1.8616)  loss_center_1_unscaled: 3.3017 (2.6866)  loss_ce_2_unscaled: 1.4146 (1.8459)  loss_center_2_unscaled: 3.3104 (2.6672)  loss_ce_3_unscaled: 1.3577 (1.8246)  loss_center_3_unscaled: 3.2658 (2.6640)  loss_ce_4_unscaled: 1.4226 (1.8252)  loss_center_4_unscaled: 3.3202 (2.6587)  time: 0.0645  data: 0.0063  max mem: 850\n",
      "Epoch: [0]  [  630/59143]  eta: 1:08:56  lr: 0.000100  class_error: 100.00  loss: 7.9391 (11.0585)  loss_ce: 1.3405 (1.8233)  loss_center: 0.0332 (0.0267)  loss_ce_0: 1.4472 (1.8803)  loss_ce_1: 1.3616 (1.8543)  loss_ce_2: 1.3316 (1.8390)  loss_ce_3: 1.3395 (1.8174)  loss_ce_4: 1.2953 (1.8176)  loss_ce_unscaled: 1.3405 (1.8233)  class_error_unscaled: 77.7778 (82.3956)  loss_center_unscaled: 3.3232 (2.6670)  loss_ce_0_unscaled: 1.4472 (1.8803)  loss_center_0_unscaled: 3.3309 (2.6670)  loss_ce_1_unscaled: 1.3616 (1.8543)  loss_center_1_unscaled: 3.3909 (2.6987)  loss_ce_2_unscaled: 1.3316 (1.8390)  loss_center_2_unscaled: 3.4024 (2.6792)  loss_ce_3_unscaled: 1.3395 (1.8174)  loss_center_3_unscaled: 3.3427 (2.6761)  loss_ce_4_unscaled: 1.2953 (1.8176)  loss_center_4_unscaled: 3.2820 (2.6701)  time: 0.0630  data: 0.0053  max mem: 850\n",
      "Epoch: [0]  [  640/59143]  eta: 1:08:46  lr: 0.000100  class_error: 85.71  loss: 7.8306 (11.0428)  loss_ce: 1.3405 (1.8200)  loss_center: 0.0339 (0.0268)  loss_ce_0: 1.4244 (1.8783)  loss_ce_1: 1.3616 (1.8525)  loss_ce_2: 1.3043 (1.8362)  loss_ce_3: 1.3395 (1.8145)  loss_ce_4: 1.2953 (1.8145)  loss_ce_unscaled: 1.3405 (1.8200)  class_error_unscaled: 70.0000 (82.1822)  loss_center_unscaled: 3.3944 (2.6783)  loss_ce_0_unscaled: 1.4244 (1.8783)  loss_center_0_unscaled: 3.4321 (2.6779)  loss_ce_1_unscaled: 1.3616 (1.8525)  loss_center_1_unscaled: 3.4691 (2.7101)  loss_ce_2_unscaled: 1.3043 (1.8362)  loss_center_2_unscaled: 3.4909 (2.6905)  loss_ce_3_unscaled: 1.3395 (1.8145)  loss_center_3_unscaled: 3.4690 (2.6877)  loss_ce_4_unscaled: 1.2953 (1.8145)  loss_center_4_unscaled: 3.4608 (2.6815)  time: 0.0603  data: 0.0064  max mem: 850\n",
      "Epoch: [0]  [  650/59143]  eta: 1:08:37  lr: 0.000100  class_error: 75.00  loss: 6.4256 (10.9713)  loss_ce: 1.0159 (1.8075)  loss_center: 0.0340 (0.0269)  loss_ce_0: 1.1320 (1.8672)  loss_ce_1: 1.1533 (1.8415)  loss_ce_2: 1.0998 (1.8244)  loss_ce_3: 1.0274 (1.8020)  loss_ce_4: 1.0061 (1.8017)  loss_ce_unscaled: 1.0159 (1.8075)  class_error_unscaled: 66.6667 (81.8584)  loss_center_unscaled: 3.4047 (2.6898)  loss_ce_0_unscaled: 1.1320 (1.8672)  loss_center_0_unscaled: 3.4321 (2.6897)  loss_ce_1_unscaled: 1.1533 (1.8415)  loss_center_1_unscaled: 3.5050 (2.7227)  loss_ce_2_unscaled: 1.0998 (1.8244)  loss_center_2_unscaled: 3.4693 (2.7023)  loss_ce_3_unscaled: 1.0274 (1.8020)  loss_center_3_unscaled: 3.4697 (2.6996)  loss_ce_4_unscaled: 1.0061 (1.8017)  loss_center_4_unscaled: 3.4279 (2.6925)  time: 0.0612  data: 0.0065  max mem: 852\n",
      "Epoch: [0]  [  660/59143]  eta: 1:08:27  lr: 0.000100  class_error: 53.85  loss: 7.0686 (10.9340)  loss_ce: 1.1358 (1.8012)  loss_center: 0.0349 (0.0270)  loss_ce_0: 1.2825 (1.8616)  loss_ce_1: 1.1997 (1.8354)  loss_ce_2: 1.1679 (1.8179)  loss_ce_3: 1.1325 (1.7954)  loss_ce_4: 1.1548 (1.7955)  loss_ce_unscaled: 1.1358 (1.8012)  class_error_unscaled: 60.0000 (81.5974)  loss_center_unscaled: 3.4927 (2.7024)  loss_ce_0_unscaled: 1.2825 (1.8616)  loss_center_0_unscaled: 3.5091 (2.7015)  loss_ce_1_unscaled: 1.1997 (1.8354)  loss_center_1_unscaled: 3.5950 (2.7349)  loss_ce_2_unscaled: 1.1679 (1.8179)  loss_center_2_unscaled: 3.4708 (2.7145)  loss_ce_3_unscaled: 1.1325 (1.7954)  loss_center_3_unscaled: 3.4807 (2.7116)  loss_ce_4_unscaled: 1.1548 (1.7955)  loss_center_4_unscaled: 3.4888 (2.7049)  time: 0.0607  data: 0.0062  max mem: 852\n",
      "Epoch: [0]  [  670/59143]  eta: 1:08:21  lr: 0.000100  class_error: 27.27  loss: 8.2194 (10.9141)  loss_ce: 1.3330 (1.7978)  loss_center: 0.0346 (0.0271)  loss_ce_0: 1.4907 (1.8587)  loss_ce_1: 1.4014 (1.8318)  loss_ce_2: 1.3674 (1.8147)  loss_ce_3: 1.3259 (1.7920)  loss_ce_4: 1.3561 (1.7921)  loss_ce_unscaled: 1.3330 (1.7978)  class_error_unscaled: 66.6667 (81.4407)  loss_center_unscaled: 3.4643 (2.7127)  loss_ce_0_unscaled: 1.4907 (1.8587)  loss_center_0_unscaled: 3.5091 (2.7129)  loss_ce_1_unscaled: 1.4014 (1.8318)  loss_center_1_unscaled: 3.5271 (2.7465)  loss_ce_2_unscaled: 1.3674 (1.8147)  loss_center_2_unscaled: 3.4708 (2.7252)  loss_ce_3_unscaled: 1.3259 (1.7920)  loss_center_3_unscaled: 3.4710 (2.7222)  loss_ce_4_unscaled: 1.3561 (1.7921)  loss_center_4_unscaled: 3.4597 (2.7152)  time: 0.0617  data: 0.0066  max mem: 852\n",
      "Epoch: [0]  [  680/59143]  eta: 1:08:10  lr: 0.000100  class_error: 71.43  loss: 8.0242 (10.9038)  loss_ce: 1.2861 (1.7956)  loss_center: 0.0338 (0.0272)  loss_ce_0: 1.4073 (1.8574)  loss_ce_1: 1.3150 (1.8302)  loss_ce_2: 1.3470 (1.8131)  loss_ce_3: 1.1968 (1.7901)  loss_ce_4: 1.2888 (1.7901)  loss_ce_unscaled: 1.2861 (1.7956)  class_error_unscaled: 79.4118 (81.4489)  loss_center_unscaled: 3.3844 (2.7232)  loss_ce_0_unscaled: 1.4073 (1.8574)  loss_center_0_unscaled: 3.4312 (2.7223)  loss_ce_1_unscaled: 1.3150 (1.8302)  loss_center_1_unscaled: 3.5199 (2.7570)  loss_ce_2_unscaled: 1.3470 (1.8131)  loss_center_2_unscaled: 3.4228 (2.7359)  loss_ce_3_unscaled: 1.1968 (1.7901)  loss_center_3_unscaled: 3.4551 (2.7329)  loss_ce_4_unscaled: 1.2888 (1.7901)  loss_center_4_unscaled: 3.3982 (2.7259)  time: 0.0611  data: 0.0065  max mem: 852\n",
      "Epoch: [0]  [  690/59143]  eta: 1:08:02  lr: 0.000100  class_error: 77.78  loss: 7.2996 (10.8843)  loss_ce: 1.2025 (1.7924)  loss_center: 0.0360 (0.0274)  loss_ce_0: 1.3452 (1.8546)  loss_ce_1: 1.2522 (1.8270)  loss_ce_2: 1.1906 (1.8096)  loss_ce_3: 1.1882 (1.7866)  loss_ce_4: 1.1136 (1.7866)  loss_ce_unscaled: 1.2025 (1.7924)  class_error_unscaled: 77.7778 (81.3624)  loss_center_unscaled: 3.6000 (2.7383)  loss_ce_0_unscaled: 1.3452 (1.8546)  loss_center_0_unscaled: 3.5493 (2.7358)  loss_ce_1_unscaled: 1.2522 (1.8270)  loss_center_1_unscaled: 3.6043 (2.7717)  loss_ce_2_unscaled: 1.1906 (1.8096)  loss_center_2_unscaled: 3.5841 (2.7501)  loss_ce_3_unscaled: 1.1882 (1.7866)  loss_center_3_unscaled: 3.6019 (2.7473)  loss_ce_4_unscaled: 1.1136 (1.7866)  loss_center_4_unscaled: 3.5703 (2.7408)  time: 0.0599  data: 0.0072  max mem: 852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  700/59143]  eta: 1:07:51  lr: 0.000100  class_error: 100.00  loss: 6.8082 (10.8369)  loss_ce: 1.0559 (1.7842)  loss_center: 0.0366 (0.0275)  loss_ce_0: 1.1994 (1.8467)  loss_ce_1: 1.2522 (1.8194)  loss_ce_2: 1.1410 (1.8017)  loss_ce_3: 1.0702 (1.7791)  loss_ce_4: 1.0140 (1.7783)  loss_ce_unscaled: 1.0559 (1.7842)  class_error_unscaled: 75.0000 (81.2764)  loss_center_unscaled: 3.6621 (2.7491)  loss_ce_0_unscaled: 1.1994 (1.8467)  loss_center_0_unscaled: 3.5678 (2.7459)  loss_ce_1_unscaled: 1.2522 (1.8194)  loss_center_1_unscaled: 3.6848 (2.7831)  loss_ce_2_unscaled: 1.1410 (1.8017)  loss_center_2_unscaled: 3.6448 (2.7612)  loss_ce_3_unscaled: 1.0702 (1.7791)  loss_center_3_unscaled: 3.6260 (2.7596)  loss_ce_4_unscaled: 1.0140 (1.7783)  loss_center_4_unscaled: 3.5719 (2.7512)  time: 0.0594  data: 0.0064  max mem: 852\n",
      "Epoch: [0]  [  710/59143]  eta: 1:07:42  lr: 0.000100  class_error: 75.00  loss: 7.1013 (10.8142)  loss_ce: 1.1645 (1.7804)  loss_center: 0.0355 (0.0276)  loss_ce_0: 1.2402 (1.8432)  loss_ce_1: 1.2949 (1.8155)  loss_ce_2: 1.1410 (1.7983)  loss_ce_3: 1.2462 (1.7749)  loss_ce_4: 1.0735 (1.7743)  loss_ce_unscaled: 1.1645 (1.7804)  class_error_unscaled: 75.0000 (81.1349)  loss_center_unscaled: 3.5489 (2.7614)  loss_ce_0_unscaled: 1.2402 (1.8432)  loss_center_0_unscaled: 3.4764 (2.7569)  loss_ce_1_unscaled: 1.2949 (1.8155)  loss_center_1_unscaled: 3.5525 (2.7948)  loss_ce_2_unscaled: 1.1410 (1.7983)  loss_center_2_unscaled: 3.5781 (2.7735)  loss_ce_3_unscaled: 1.2462 (1.7749)  loss_center_3_unscaled: 3.5737 (2.7716)  loss_ce_4_unscaled: 1.0735 (1.7743)  loss_center_4_unscaled: 3.4942 (2.7635)  time: 0.0584  data: 0.0053  max mem: 852\n",
      "Epoch: [0]  [  720/59143]  eta: 1:07:32  lr: 0.000100  class_error: 84.21  loss: 7.1039 (10.7723)  loss_ce: 1.1325 (1.7728)  loss_center: 0.0358 (0.0277)  loss_ce_0: 1.2587 (1.8370)  loss_ce_1: 1.2139 (1.8093)  loss_ce_2: 1.2195 (1.7913)  loss_ce_3: 1.1646 (1.7676)  loss_ce_4: 1.1192 (1.7666)  loss_ce_unscaled: 1.1325 (1.7728)  class_error_unscaled: 83.3333 (81.0383)  loss_center_unscaled: 3.5818 (2.7725)  loss_ce_0_unscaled: 1.2587 (1.8370)  loss_center_0_unscaled: 3.5509 (2.7683)  loss_ce_1_unscaled: 1.2139 (1.8093)  loss_center_1_unscaled: 3.6526 (2.8070)  loss_ce_2_unscaled: 1.2195 (1.7913)  loss_center_2_unscaled: 3.5781 (2.7854)  loss_ce_3_unscaled: 1.1646 (1.7676)  loss_center_3_unscaled: 3.5783 (2.7833)  loss_ce_4_unscaled: 1.1192 (1.7666)  loss_center_4_unscaled: 3.5862 (2.7747)  time: 0.0585  data: 0.0061  max mem: 852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                   | 0/10 [00:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdistributed:\n\u001b[1;32m     12\u001b[0m     sampler_train\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[0;32m---> 13\u001b[0m train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_max_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39moutput_dir:\n",
      "File \u001b[0;32m~/detr_finetune/engine.py:32\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[1;32m     29\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m---> 32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     34\u001b[0m weight_dict \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mweight_dict\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/detr_finetune/models/detr_robust.py:67\u001b[0m, in \u001b[0;36mModified_DETR.forward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(samples, (\u001b[38;5;28mlist\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor)):\n\u001b[1;32m     66\u001b[0m     samples \u001b[38;5;241m=\u001b[39m nested_tensor_from_tensor_list(samples)\n\u001b[0;32m---> 67\u001b[0m features, pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m src, mask \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecompose()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/detr_finetune/models/backbone.py:101\u001b[0m, in \u001b[0;36mJoiner.forward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_list: NestedTensor):\n\u001b[0;32m--> 101\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     out: List[NestedTensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    103\u001b[0m     pos \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/detr_finetune/models/backbone.py:73\u001b[0m, in \u001b[0;36mBackboneBase.forward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_list: NestedTensor):\n\u001b[0;32m---> 73\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     out: Dict[\u001b[38;5;28mstr\u001b[39m, NestedTensor] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, x \u001b[38;5;129;01min\u001b[39;00m xs\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m out \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers:\n\u001b[1;32m     71\u001b[0m         out_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/resnet.py:152\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m--> 152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[1;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1498\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m-> 1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args.eval:\n",
    "    test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                          data_loader_val, base_ds, device, args.output_dir)\n",
    "    if args.output_dir:\n",
    "        utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "    \n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(args.start_epoch, args.epochs)):\n",
    "    if args.distributed:\n",
    "        sampler_train.set_epoch(epoch)\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "        args.clip_max_norm)\n",
    "    lr_scheduler.step()\n",
    "    if args.output_dir:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 100 epochs\n",
    "        if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 1 == 0:                 # original: (epoch + 1) % 100\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    test_stats, coco_evaluator = evaluate(\n",
    "        model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
    "    )\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                 **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                 'epoch': epoch,\n",
    "                 'n_parameters': n_parameters}\n",
    "\n",
    "    if args.output_dir and utils.is_main_process():\n",
    "        with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "        # for evaluation logs\n",
    "        if coco_evaluator is not None:\n",
    "            (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "            if \"bbox\" in coco_evaluator.coco_eval:\n",
    "                filenames = ['latest.pth']\n",
    "                if epoch % 50 == 0:\n",
    "                    filenames.append(f'{epoch:03}.pth')\n",
    "                for name in filenames:\n",
    "                    torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "                               output_dir / \"eval\" / name)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a7dc77-1dac-4c2f-a5e6-af2a8d30616d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0493,  0.0418,  0.0334,  0.0225,  0.0373,  0.0177,  0.0307,  0.0117,\n",
       "         0.0466,  0.0503,  0.0287,  0.0184,  0.0429,  0.0229,  0.0382,  0.0135,\n",
       "         0.0389,  0.0164,  0.0227,  0.0522,  0.0467,  0.0428,  0.0323,  0.0227,\n",
       "         0.0495,  0.0035,  0.0387,  0.0611,  0.0107, -0.0036,  0.0350,  0.0375,\n",
       "         0.0243,  0.0307,  0.0131,  0.0280,  0.0441,  0.0437,  0.0544,  0.0347,\n",
       "         0.0440,  0.0024,  0.0245,  0.0317,  0.0275,  0.0205,  0.0376,  0.0561,\n",
       "         0.0469,  0.0524,  0.0473,  0.0299,  0.0579,  0.0337,  0.0321,  0.0401,\n",
       "         0.0213,  0.0312,  0.0257,  0.0391,  0.0199,  0.0137,  0.0510,  0.0134,\n",
       "         0.0303,  0.0086,  0.0352,  0.0094,  0.0260,  0.0238,  0.0245,  0.0134,\n",
       "         0.0176,  0.0412,  0.0205,  0.0382,  0.0400,  0.0614,  0.0424,  0.0338,\n",
       "         0.0442,  0.0542,  0.0532,  0.0012,  0.0647,  0.0511,  0.0102,  0.0437,\n",
       "        -0.0213,  0.0152,  0.0064,  0.0065,  0.0279,  0.0163,  0.0637,  0.0505,\n",
       "         0.0130,  0.0234,  0.0657,  0.0468,  0.0475,  0.0408,  0.0289,  0.0297,\n",
       "         0.0238,  0.0379,  0.0212,  0.0262,  0.0180,  0.0507,  0.0637,  0.0473,\n",
       "         0.0198,  0.0443,  0.0377,  0.0332,  0.0155,  0.0120,  0.0225,  0.0345,\n",
       "         0.0285,  0.0380,  0.0694,  0.0558,  0.0441,  0.0385,  0.0339,  0.0311])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint0000= torch.load('/home/anazeri/detr_finetune/detr-r50-coco-modifhead-128fc92fc-TEMP/checkpoint0000.pth', \n",
    "           map_location = 'cpu')\n",
    "checkpoint0000['model']['class_embed.0.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef57793-f1d2-4f6b-99a7-89fa8f4a0bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0383, -0.0151,  0.0174,  0.0203, -0.0082, -0.0136,  0.0467, -0.0315,\n",
       "         0.0432,  0.0296,  0.0200, -0.0265,  0.0202, -0.0029,  0.0249, -0.0037,\n",
       "        -0.0043, -0.0083, -0.0289,  0.0454, -0.0078,  0.0072,  0.0042, -0.0131,\n",
       "         0.0441,  0.0098, -0.0198,  0.0711, -0.0446, -0.0542,  0.0385,  0.0290,\n",
       "         0.0085, -0.0139, -0.0312, -0.0041,  0.0351,  0.0458,  0.0585,  0.0162,\n",
       "         0.0014, -0.0529, -0.0242, -0.0002,  0.0203, -0.0157,  0.0068,  0.0674,\n",
       "         0.0019,  0.0373,  0.0073,  0.0351,  0.0220,  0.0068, -0.0129,  0.0121,\n",
       "         0.0170,  0.0086, -0.0027,  0.0061, -0.0133, -0.0092,  0.0280, -0.0384,\n",
       "         0.0100, -0.0195,  0.0180, -0.0440, -0.0198,  0.0262, -0.0415, -0.0782,\n",
       "        -0.0258,  0.0234, -0.0325,  0.0171,  0.0358,  0.0410,  0.0462,  0.0115,\n",
       "         0.0056,  0.0532,  0.0406, -0.0442,  0.0438,  0.0294, -0.0183, -0.0020,\n",
       "        -0.0376, -0.0297, -0.0507, -0.0444,  0.0179, -0.0309,  0.0382,  0.0063,\n",
       "         0.0132,  0.0020,  0.0384,  0.0461, -0.0097,  0.0173,  0.0266,  0.0023,\n",
       "        -0.0279, -0.0063, -0.0387, -0.0241,  0.0150,  0.0603,  0.0468,  0.0299,\n",
       "        -0.0207,  0.0258,  0.0024, -0.0226, -0.0391, -0.0363,  0.0189,  0.0394,\n",
       "         0.0212, -0.0131,  0.0390,  0.0075,  0.0031,  0.0561, -0.0212,  0.0317])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint0009= torch.load('/home/anazeri/detr_finetune/detr-r50-coco-modifhead-128fc92fc-TEMP/checkpoint0009.pth', \n",
    "           map_location = 'cpu')\n",
    "checkpoint0009['model']['class_embed.0.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53dbfa75-4bf2-4d59-8588-d2e09aae9bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_without_ddp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m####################### @amirhnazerii #######################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m##### start 03/28/2025\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## Freeze all model parameters except the classification head\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_without_ddp\u001b[49m\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m      5\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model_without_ddp\u001b[38;5;241m.\u001b[39mclass_embed\u001b[38;5;241m.\u001b[39mparameters():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_without_ddp' is not defined"
     ]
    }
   ],
   "source": [
    "####################### @amirhnazerii #######################\n",
    "##### start 03/28/2025\n",
    "## Freeze all model parameters except the classification head\n",
    "for param in model_without_ddp.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_without_ddp.class_embed.parameters():\n",
    "    param.requires_grad = True  # Only train the classification head\n",
    "\n",
    "# Re-define optimizer (only updates classification head parameters)\n",
    "optimizer = torch.optim.AdamW(model_without_ddp.class_embed.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "#     optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "#                                   weight_decay=args.weight_decay)\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model_without_ddp.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "##### end   \n",
    "\n",
    "\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "dataset_train = build_dataset(image_set='train', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "if args.distributed:\n",
    "    sampler_train = DistributedSampler(dataset_train)\n",
    "    sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "else:\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                               collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                             drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "\n",
    "\n",
    "###### END of modification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc41f0a5-d8cf-4bed-8441-9af4fb908af3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=300, lr_drop=200, clip_max_norm=0.1, num_classes=None, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, pre_norm=False, new_layer_dim=128, masks=False, aux_loss=True, set_cost_class=1, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, dataset_file='coco', coco_path='/scratch/anazeri/coco/', coco_panoptic_path=None, remove_difficult=False, output_dir='', device='cuda', seed=42, resume='detr-r50-modifhead-128fc92fc.pth', start_epoch=0, eval=False, num_workers=2, world_size=1, dist_url='env://', distributed=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if args.world_size > 1:\n",
    "    utils.init_distributed_mode(args)  # Enable distributed mode if running on multiple GPUs\n",
    "    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "else:\n",
    "    args.distributed = False  # Force single GPU mode\n",
    "\n",
    "\n",
    "print(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "# if args.distributed:\n",
    "#     model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "#     model_without_ddp = model.module\n",
    "\n",
    "# n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print('number of params:', n_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782bcdb9-5634-4643-a67b-6ad673b390be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.dataset_file == \"coco_panoptic\":\n",
    "    # We also evaluate AP during panoptic training, on original coco DS\n",
    "    coco_val = datasets.coco.build(\"val\", args)\n",
    "    base_ds = get_coco_api_from_dataset(coco_val)\n",
    "else:\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "output_dir = Path(args.output_dir)\n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model_without_ddp.load_state_dict(checkpoint['model'], strict=False)  #### Modified by Amir: , strict=False\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "if args.eval:\n",
    "    test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                          data_loader_val, base_ds, device, args.output_dir)\n",
    "    if args.output_dir:\n",
    "        utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c084ff-f107-43c3-891c-7d451819d1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Modified_DETR(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_embed): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=92, bias=True)\n",
       "  )\n",
       "  (bbox_embed): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (query_embed): Embedding(100, 256)\n",
       "  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (backbone): Joiner(\n",
       "    (0): Backbone(\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d()\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PositionEmbeddingSine()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_without_ddp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Detr_env1",
   "language": "python",
   "name": "detr_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
