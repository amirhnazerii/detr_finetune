{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7926fc3-6925-43a0-8900-9f97e5cedc56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from engine import evaluate, train_one_epoch\n",
    "from models import build_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7315bd1a-ecc2-449b-b2f3-a6aec6c2f7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--num_classes', type=int, default=None,\n",
    "                        help=\"Number of classes in dataset+1\")\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "\n",
    "    \n",
    "    ####################### @amirhnazerii #######################\n",
    "    ##### start 03/27/2025\n",
    "    # * Classification head\n",
    "    parser.add_argument('--new_layer_dim', default=None, type=int,\n",
    "                        help=\"classification head added fc-layer dim\")\n",
    "    \n",
    "    ##### end 03/27/2025\n",
    "    \n",
    "    \n",
    "    # * Segmentation\n",
    "    parser.add_argument('--masks', action='store_true',\n",
    "                        help=\"Train segmentation head if the flag is provided\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--inter_class_weight', default=None, type=float,\n",
    "                    help=\"Weight for inter-class distance maximization in CenterLoss - amirhnazerii 5/2/2025\")\n",
    "\n",
    "    \n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str)\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "    \n",
    "    \n",
    "    # robustness param:\n",
    "    parser.add_argument('--robust', default=False, type=bool,\n",
    "                        help='nrobust detr training with modified loss function.')\n",
    "    \n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34599056-9fa9-46a2-95f3-43e84df5d3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n",
    "args = parser.parse_args(args=['--resume', 'detr-r50-modifhead-128fc92fc.pth',\n",
    "                              '--coco_path', '/scratch/anazeri/coco/',\n",
    "                               '--output_dir' ,'/home/anazeri/detr_finetune/robust-detr-r50-coco-modifhead-128fc92fc-TEMP',\n",
    "                               \"--lr_drop\", \"7\",  \n",
    "                              '--new_layer_dim', '128',\n",
    "                               \"--inter_class_weight\",\"0\",\n",
    "                              \"--robust\", \"True\",\n",
    "                               \"--epochs\", \"10\"\n",
    "                              ])\n",
    "\n",
    "args.distributed = False\n",
    "# --dataset_file \"coco\" \\\n",
    "    # --coco_path \"/scratch/anazeri/coco/\" \\\n",
    "    # --output_dir \"/home/anazeri/detr_finetune/detr-r50-coco-modifhead-128fc92fc-epoch10\" \\\n",
    "    # --resume \"detr-r50-modifhead-128fc92fc.pth\" \\\n",
    "    # --lr_drop 3 \\\n",
    "    # --backbone \"resnet50\" \\\n",
    "    # --epochs 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2058ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=10, lr_drop=7, clip_max_norm=0.1, num_classes=None, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, pre_norm=False, new_layer_dim=128, masks=False, aux_loss=True, inter_class_weight=0.0, set_cost_class=1, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, dataset_file='coco', coco_path='/scratch/anazeri/coco/', coco_panoptic_path=None, remove_difficult=False, output_dir='/home/anazeri/detr_finetune/robust-detr-r50-coco-modifhead-128fc92fc-TEMP', device='cuda', seed=42, resume='detr-r50-modifhead-128fc92fc.pth', start_epoch=0, eval=False, num_workers=2, world_size=1, dist_url='env://', robust=True, distributed=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if args.world_size > 1:\n",
    "    utils.init_distributed_mode(args)  # Enable distributed mode if running on multiple GPUs\n",
    "    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "else:\n",
    "    args.distributed = False  # Force single GPU mode\n",
    "\n",
    "\n",
    "print(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0872dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 44764\n",
      "loading annotations into memory...\n",
      "Done (t=10.54s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.34s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "####################### @amirhnazerii #######################\n",
    "##### start 03/28/2025\n",
    "## Freeze all model parameters except the classification head\n",
    "for param in model_without_ddp.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_without_ddp.class_embed.parameters():\n",
    "    param.requires_grad = True  # Only train the classification head\n",
    "\n",
    "# Re-define optimizer (only updates classification head parameters)\n",
    "optimizer = torch.optim.AdamW(model_without_ddp.class_embed.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "#     optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "#                                   weight_decay=args.weight_decay)\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model_without_ddp.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "##### end   \n",
    "\n",
    "\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "dataset_train = build_dataset(image_set='train', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "if args.distributed:\n",
    "    sampler_train = DistributedSampler(dataset_train)\n",
    "    sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "else:\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                               collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                             drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if args.dataset_file == \"coco_panoptic\":\n",
    "    # We also evaluate AP during panoptic training, on original coco DS\n",
    "    coco_val = datasets.coco.build(\"val\", args)\n",
    "    base_ds = get_coco_api_from_dataset(coco_val)\n",
    "else:\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "output_dir = Path(args.output_dir)\n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model_without_ddp.load_state_dict(checkpoint['model'], strict=False)  #### Modified by Amir: , strict=False\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a61c849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                   | 0/10 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Only 'labels' loss should be used, got boxes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdistributed:\n\u001b[1;32m     12\u001b[0m     sampler_train\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[0;32m---> 13\u001b[0m train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_max_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39moutput_dir:\n",
      "File \u001b[0;32m~/detr_finetune/engine.py:33\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[1;32m     30\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m     32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(samples)\n\u001b[0;32m---> 33\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m weight_dict \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mweight_dict\n\u001b[1;32m     35\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss_dict[k] \u001b[38;5;241m*\u001b[39m weight_dict[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m weight_dict)\n",
      "File \u001b[0;32m~/.conda/envs/Detr_env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/detr_finetune/models/detr_robust.py:159\u001b[0m, in \u001b[0;36mSetCriterion.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    157\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses:\n\u001b[0;32m--> 159\u001b[0m     losses\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boxes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maux_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, aux_outputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maux_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "File \u001b[0;32m~/detr_finetune/models/detr_robust.py:143\u001b[0m, in \u001b[0;36mSetCriterion.get_loss\u001b[0;34m(self, loss, outputs, targets, indices, num_boxes, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, outputs, targets, indices, num_boxes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m loss \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m loss should be used, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_labels(outputs, targets, indices, num_boxes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Only 'labels' loss should be used, got boxes"
     ]
    }
   ],
   "source": [
    "if args.eval:\n",
    "    test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                          data_loader_val, base_ds, device, args.output_dir)\n",
    "    if args.output_dir:\n",
    "        utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "    \n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(args.start_epoch, args.epochs)):\n",
    "    if args.distributed:\n",
    "        sampler_train.set_epoch(epoch)\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "        args.clip_max_norm)\n",
    "    lr_scheduler.step()\n",
    "    if args.output_dir:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 100 epochs\n",
    "        if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 1 == 0:                 # original: (epoch + 1) % 100\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    test_stats, coco_evaluator = evaluate(\n",
    "        model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
    "    )\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                 **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                 'epoch': epoch,\n",
    "                 'n_parameters': n_parameters}\n",
    "\n",
    "    if args.output_dir and utils.is_main_process():\n",
    "        with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "        # for evaluation logs\n",
    "        if coco_evaluator is not None:\n",
    "            (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "            if \"bbox\" in coco_evaluator.coco_eval:\n",
    "                filenames = ['latest.pth']\n",
    "                if epoch % 50 == 0:\n",
    "                    filenames.append(f'{epoch:03}.pth')\n",
    "                for name in filenames:\n",
    "                    torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "                               output_dir / \"eval\" / name)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a7dc77-1dac-4c2f-a5e6-af2a8d30616d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0493,  0.0418,  0.0334,  0.0225,  0.0373,  0.0177,  0.0307,  0.0117,\n",
       "         0.0466,  0.0503,  0.0287,  0.0184,  0.0429,  0.0229,  0.0382,  0.0135,\n",
       "         0.0389,  0.0164,  0.0227,  0.0522,  0.0467,  0.0428,  0.0323,  0.0227,\n",
       "         0.0495,  0.0035,  0.0387,  0.0611,  0.0107, -0.0036,  0.0350,  0.0375,\n",
       "         0.0243,  0.0307,  0.0131,  0.0280,  0.0441,  0.0437,  0.0544,  0.0347,\n",
       "         0.0440,  0.0024,  0.0245,  0.0317,  0.0275,  0.0205,  0.0376,  0.0561,\n",
       "         0.0469,  0.0524,  0.0473,  0.0299,  0.0579,  0.0337,  0.0321,  0.0401,\n",
       "         0.0213,  0.0312,  0.0257,  0.0391,  0.0199,  0.0137,  0.0510,  0.0134,\n",
       "         0.0303,  0.0086,  0.0352,  0.0094,  0.0260,  0.0238,  0.0245,  0.0134,\n",
       "         0.0176,  0.0412,  0.0205,  0.0382,  0.0400,  0.0614,  0.0424,  0.0338,\n",
       "         0.0442,  0.0542,  0.0532,  0.0012,  0.0647,  0.0511,  0.0102,  0.0437,\n",
       "        -0.0213,  0.0152,  0.0064,  0.0065,  0.0279,  0.0163,  0.0637,  0.0505,\n",
       "         0.0130,  0.0234,  0.0657,  0.0468,  0.0475,  0.0408,  0.0289,  0.0297,\n",
       "         0.0238,  0.0379,  0.0212,  0.0262,  0.0180,  0.0507,  0.0637,  0.0473,\n",
       "         0.0198,  0.0443,  0.0377,  0.0332,  0.0155,  0.0120,  0.0225,  0.0345,\n",
       "         0.0285,  0.0380,  0.0694,  0.0558,  0.0441,  0.0385,  0.0339,  0.0311])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint0000= torch.load('/home/anazeri/detr_finetune/detr-r50-coco-modifhead-128fc92fc-TEMP/checkpoint0000.pth', \n",
    "           map_location = 'cpu')\n",
    "checkpoint0000['model']['class_embed.0.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef57793-f1d2-4f6b-99a7-89fa8f4a0bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0383, -0.0151,  0.0174,  0.0203, -0.0082, -0.0136,  0.0467, -0.0315,\n",
       "         0.0432,  0.0296,  0.0200, -0.0265,  0.0202, -0.0029,  0.0249, -0.0037,\n",
       "        -0.0043, -0.0083, -0.0289,  0.0454, -0.0078,  0.0072,  0.0042, -0.0131,\n",
       "         0.0441,  0.0098, -0.0198,  0.0711, -0.0446, -0.0542,  0.0385,  0.0290,\n",
       "         0.0085, -0.0139, -0.0312, -0.0041,  0.0351,  0.0458,  0.0585,  0.0162,\n",
       "         0.0014, -0.0529, -0.0242, -0.0002,  0.0203, -0.0157,  0.0068,  0.0674,\n",
       "         0.0019,  0.0373,  0.0073,  0.0351,  0.0220,  0.0068, -0.0129,  0.0121,\n",
       "         0.0170,  0.0086, -0.0027,  0.0061, -0.0133, -0.0092,  0.0280, -0.0384,\n",
       "         0.0100, -0.0195,  0.0180, -0.0440, -0.0198,  0.0262, -0.0415, -0.0782,\n",
       "        -0.0258,  0.0234, -0.0325,  0.0171,  0.0358,  0.0410,  0.0462,  0.0115,\n",
       "         0.0056,  0.0532,  0.0406, -0.0442,  0.0438,  0.0294, -0.0183, -0.0020,\n",
       "        -0.0376, -0.0297, -0.0507, -0.0444,  0.0179, -0.0309,  0.0382,  0.0063,\n",
       "         0.0132,  0.0020,  0.0384,  0.0461, -0.0097,  0.0173,  0.0266,  0.0023,\n",
       "        -0.0279, -0.0063, -0.0387, -0.0241,  0.0150,  0.0603,  0.0468,  0.0299,\n",
       "        -0.0207,  0.0258,  0.0024, -0.0226, -0.0391, -0.0363,  0.0189,  0.0394,\n",
       "         0.0212, -0.0131,  0.0390,  0.0075,  0.0031,  0.0561, -0.0212,  0.0317])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint0009= torch.load('/home/anazeri/detr_finetune/detr-r50-coco-modifhead-128fc92fc-TEMP/checkpoint0009.pth', \n",
    "           map_location = 'cpu')\n",
    "checkpoint0009['model']['class_embed.0.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53dbfa75-4bf2-4d59-8588-d2e09aae9bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_without_ddp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m####################### @amirhnazerii #######################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m##### start 03/28/2025\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## Freeze all model parameters except the classification head\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_without_ddp\u001b[49m\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m      5\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model_without_ddp\u001b[38;5;241m.\u001b[39mclass_embed\u001b[38;5;241m.\u001b[39mparameters():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_without_ddp' is not defined"
     ]
    }
   ],
   "source": [
    "####################### @amirhnazerii #######################\n",
    "##### start 03/28/2025\n",
    "## Freeze all model parameters except the classification head\n",
    "for param in model_without_ddp.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_without_ddp.class_embed.parameters():\n",
    "    param.requires_grad = True  # Only train the classification head\n",
    "\n",
    "# Re-define optimizer (only updates classification head parameters)\n",
    "optimizer = torch.optim.AdamW(model_without_ddp.class_embed.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "#     optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "#                                   weight_decay=args.weight_decay)\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model_without_ddp.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "##### end   \n",
    "\n",
    "\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "dataset_train = build_dataset(image_set='train', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "if args.distributed:\n",
    "    sampler_train = DistributedSampler(dataset_train)\n",
    "    sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "else:\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                               collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                             drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "\n",
    "\n",
    "###### END of modification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc41f0a5-d8cf-4bed-8441-9af4fb908af3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=300, lr_drop=200, clip_max_norm=0.1, num_classes=None, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, pre_norm=False, new_layer_dim=128, masks=False, aux_loss=True, set_cost_class=1, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, dataset_file='coco', coco_path='/scratch/anazeri/coco/', coco_panoptic_path=None, remove_difficult=False, output_dir='', device='cuda', seed=42, resume='detr-r50-modifhead-128fc92fc.pth', start_epoch=0, eval=False, num_workers=2, world_size=1, dist_url='env://', distributed=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if args.world_size > 1:\n",
    "    utils.init_distributed_mode(args)  # Enable distributed mode if running on multiple GPUs\n",
    "    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "else:\n",
    "    args.distributed = False  # Force single GPU mode\n",
    "\n",
    "\n",
    "print(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model, criterion, postprocessors = build_model(args)\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "# if args.distributed:\n",
    "#     model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "#     model_without_ddp = model.module\n",
    "\n",
    "# n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print('number of params:', n_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782bcdb9-5634-4643-a67b-6ad673b390be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.dataset_file == \"coco_panoptic\":\n",
    "    # We also evaluate AP during panoptic training, on original coco DS\n",
    "    coco_val = datasets.coco.build(\"val\", args)\n",
    "    base_ds = get_coco_api_from_dataset(coco_val)\n",
    "else:\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "output_dir = Path(args.output_dir)\n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model_without_ddp.load_state_dict(checkpoint['model'], strict=False)  #### Modified by Amir: , strict=False\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "if args.eval:\n",
    "    test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                          data_loader_val, base_ds, device, args.output_dir)\n",
    "    if args.output_dir:\n",
    "        utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c084ff-f107-43c3-891c-7d451819d1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Modified_DETR(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_embed): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=92, bias=True)\n",
       "  )\n",
       "  (bbox_embed): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (query_embed): Embedding(100, 256)\n",
       "  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (backbone): Joiner(\n",
       "    (0): Backbone(\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d()\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PositionEmbeddingSine()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_without_ddp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Detr_env1",
   "language": "python",
   "name": "detr_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
