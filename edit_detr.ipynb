{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consecutive-confidence",
   "metadata": {},
   "source": [
    "# Prep\n",
    "\n",
    "Setting up some prior functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rational-neighborhood",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1 False\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392dac57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import util.misc as utils\n",
    "from models import build_model\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b329f5",
   "metadata": {},
   "source": [
    "# Modified DETR Architecture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a705593e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import build_model\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--num_classes', type=int, default=None,\n",
    "                        help=\"Number of classes in dataset+1\")\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "\n",
    "    ####################### @amirhnazerii #######################\n",
    "    ##### start 03/27/2025\n",
    "    # * Classification head\n",
    "    parser.add_argument('--new_layer_dim', default=None, type=int,\n",
    "                        help=\"classification head added fc-layer dim\")\n",
    "    ##### end 03/27/2025\n",
    "    \n",
    "    \n",
    "    # * Segmentation\n",
    "    parser.add_argument('--masks', action='store_true',\n",
    "                        help=\"Train segmentation head if the flag is provided\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str)\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995e3359",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = get_args_parser()\n",
    "    args = parser.parse_args(['--new_layer_dim', '128'])\n",
    "    model, _, _ = build_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1cb7da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=300, lr_drop=200, clip_max_norm=0.1, num_classes=None, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, pre_norm=False, new_layer_dim=128, masks=False, aux_loss=True, set_cost_class=1, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, dataset_file='coco', coco_path=None, coco_panoptic_path=None, remove_difficult=False, output_dir='', device='cuda', seed=42, resume='', start_epoch=0, eval=False, num_workers=2, world_size=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0104e4b6",
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Modified_DETR(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_embed): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=92, bias=True)\n",
       "  )\n",
       "  (bbox_embed): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (query_embed): Embedding(100, 256)\n",
       "  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (backbone): Joiner(\n",
       "    (0): Backbone(\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d()\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PositionEmbeddingSine()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab16fa-5888-41e4-a8d6-a3d39b54419a",
   "metadata": {},
   "source": [
    "```\n",
    "(class_embed): Sequential(\n",
    "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=128, out_features=92, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2da0bb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 256])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(model.class_embed[0].weight.shape)\n",
    "print(model.class_embed[0].bias.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-interim",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load original weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35279925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get pretrained weights\n",
    "checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',        \n",
    "            # url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n",
    "            # url='https://dl.fbaipublicfiles.com/detr/detr-r101-2c7b67e5.pth',\n",
    "            # url='https://dl.fbaipublicfiles.com/detr/detr-r101-dc5-a2e86def.pth',\n",
    "            map_location='cpu',\n",
    "            check_hash=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85be88de",
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.out_proj.weight', 'transformer.decoder.layers.1.self_attn.out_proj.bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'transformer.decoder.layers.1.linear1.weight', 'transformer.decoder.layers.1.linear1.bias', 'transformer.decoder.layers.1.linear2.weight', 'transformer.decoder.layers.1.linear2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.layers.2.self_attn.in_proj_weight', 'transformer.decoder.layers.2.self_attn.in_proj_bias', 'transformer.decoder.layers.2.self_attn.out_proj.weight', 'transformer.decoder.layers.2.self_attn.out_proj.bias', 'transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'transformer.decoder.layers.2.multihead_attn.out_proj.weight', 'transformer.decoder.layers.2.multihead_attn.out_proj.bias', 'transformer.decoder.layers.2.linear1.weight', 'transformer.decoder.layers.2.linear1.bias', 'transformer.decoder.layers.2.linear2.weight', 'transformer.decoder.layers.2.linear2.bias', 'transformer.decoder.layers.2.norm1.weight', 'transformer.decoder.layers.2.norm1.bias', 'transformer.decoder.layers.2.norm2.weight', 'transformer.decoder.layers.2.norm2.bias', 'transformer.decoder.layers.2.norm3.weight', 'transformer.decoder.layers.2.norm3.bias', 'transformer.decoder.layers.3.self_attn.in_proj_weight', 'transformer.decoder.layers.3.self_attn.in_proj_bias', 'transformer.decoder.layers.3.self_attn.out_proj.weight', 'transformer.decoder.layers.3.self_attn.out_proj.bias', 'transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'transformer.decoder.layers.3.multihead_attn.out_proj.weight', 'transformer.decoder.layers.3.multihead_attn.out_proj.bias', 'transformer.decoder.layers.3.linear1.weight', 'transformer.decoder.layers.3.linear1.bias', 'transformer.decoder.layers.3.linear2.weight', 'transformer.decoder.layers.3.linear2.bias', 'transformer.decoder.layers.3.norm1.weight', 'transformer.decoder.layers.3.norm1.bias', 'transformer.decoder.layers.3.norm2.weight', 'transformer.decoder.layers.3.norm2.bias', 'transformer.decoder.layers.3.norm3.weight', 'transformer.decoder.layers.3.norm3.bias', 'transformer.decoder.layers.4.self_attn.in_proj_weight', 'transformer.decoder.layers.4.self_attn.in_proj_bias', 'transformer.decoder.layers.4.self_attn.out_proj.weight', 'transformer.decoder.layers.4.self_attn.out_proj.bias', 'transformer.decoder.layers.4.multihead_attn.in_proj_weight', 'transformer.decoder.layers.4.multihead_attn.in_proj_bias', 'transformer.decoder.layers.4.multihead_attn.out_proj.weight', 'transformer.decoder.layers.4.multihead_attn.out_proj.bias', 'transformer.decoder.layers.4.linear1.weight', 'transformer.decoder.layers.4.linear1.bias', 'transformer.decoder.layers.4.linear2.weight', 'transformer.decoder.layers.4.linear2.bias', 'transformer.decoder.layers.4.norm1.weight', 'transformer.decoder.layers.4.norm1.bias', 'transformer.decoder.layers.4.norm2.weight', 'transformer.decoder.layers.4.norm2.bias', 'transformer.decoder.layers.4.norm3.weight', 'transformer.decoder.layers.4.norm3.bias', 'transformer.decoder.layers.5.self_attn.in_proj_weight', 'transformer.decoder.layers.5.self_attn.in_proj_bias', 'transformer.decoder.layers.5.self_attn.out_proj.weight', 'transformer.decoder.layers.5.self_attn.out_proj.bias', 'transformer.decoder.layers.5.multihead_attn.in_proj_weight', 'transformer.decoder.layers.5.multihead_attn.in_proj_bias', 'transformer.decoder.layers.5.multihead_attn.out_proj.weight', 'transformer.decoder.layers.5.multihead_attn.out_proj.bias', 'transformer.decoder.layers.5.linear1.weight', 'transformer.decoder.layers.5.linear1.bias', 'transformer.decoder.layers.5.linear2.weight', 'transformer.decoder.layers.5.linear2.bias', 'transformer.decoder.layers.5.norm1.weight', 'transformer.decoder.layers.5.norm1.bias', 'transformer.decoder.layers.5.norm2.weight', 'transformer.decoder.layers.5.norm2.bias', 'transformer.decoder.layers.5.norm3.weight', 'transformer.decoder.layers.5.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'class_embed.weight', 'class_embed.bias', 'bbox_embed.layers.0.weight', 'bbox_embed.layers.0.bias', 'bbox_embed.layers.1.weight', 'bbox_embed.layers.1.bias', 'bbox_embed.layers.2.weight', 'bbox_embed.layers.2.bias', 'query_embed.weight', 'input_proj.weight', 'input_proj.bias', 'backbone.0.body.conv1.weight', 'backbone.0.body.bn1.weight', 'backbone.0.body.bn1.bias', 'backbone.0.body.bn1.running_mean', 'backbone.0.body.bn1.running_var', 'backbone.0.body.layer1.0.conv1.weight', 'backbone.0.body.layer1.0.bn1.weight', 'backbone.0.body.layer1.0.bn1.bias', 'backbone.0.body.layer1.0.bn1.running_mean', 'backbone.0.body.layer1.0.bn1.running_var', 'backbone.0.body.layer1.0.conv2.weight', 'backbone.0.body.layer1.0.bn2.weight', 'backbone.0.body.layer1.0.bn2.bias', 'backbone.0.body.layer1.0.bn2.running_mean', 'backbone.0.body.layer1.0.bn2.running_var', 'backbone.0.body.layer1.0.conv3.weight', 'backbone.0.body.layer1.0.bn3.weight', 'backbone.0.body.layer1.0.bn3.bias', 'backbone.0.body.layer1.0.bn3.running_mean', 'backbone.0.body.layer1.0.bn3.running_var', 'backbone.0.body.layer1.0.downsample.0.weight', 'backbone.0.body.layer1.0.downsample.1.weight', 'backbone.0.body.layer1.0.downsample.1.bias', 'backbone.0.body.layer1.0.downsample.1.running_mean', 'backbone.0.body.layer1.0.downsample.1.running_var', 'backbone.0.body.layer1.1.conv1.weight', 'backbone.0.body.layer1.1.bn1.weight', 'backbone.0.body.layer1.1.bn1.bias', 'backbone.0.body.layer1.1.bn1.running_mean', 'backbone.0.body.layer1.1.bn1.running_var', 'backbone.0.body.layer1.1.conv2.weight', 'backbone.0.body.layer1.1.bn2.weight', 'backbone.0.body.layer1.1.bn2.bias', 'backbone.0.body.layer1.1.bn2.running_mean', 'backbone.0.body.layer1.1.bn2.running_var', 'backbone.0.body.layer1.1.conv3.weight', 'backbone.0.body.layer1.1.bn3.weight', 'backbone.0.body.layer1.1.bn3.bias', 'backbone.0.body.layer1.1.bn3.running_mean', 'backbone.0.body.layer1.1.bn3.running_var', 'backbone.0.body.layer1.2.conv1.weight', 'backbone.0.body.layer1.2.bn1.weight', 'backbone.0.body.layer1.2.bn1.bias', 'backbone.0.body.layer1.2.bn1.running_mean', 'backbone.0.body.layer1.2.bn1.running_var', 'backbone.0.body.layer1.2.conv2.weight', 'backbone.0.body.layer1.2.bn2.weight', 'backbone.0.body.layer1.2.bn2.bias', 'backbone.0.body.layer1.2.bn2.running_mean', 'backbone.0.body.layer1.2.bn2.running_var', 'backbone.0.body.layer1.2.conv3.weight', 'backbone.0.body.layer1.2.bn3.weight', 'backbone.0.body.layer1.2.bn3.bias', 'backbone.0.body.layer1.2.bn3.running_mean', 'backbone.0.body.layer1.2.bn3.running_var', 'backbone.0.body.layer2.0.conv1.weight', 'backbone.0.body.layer2.0.bn1.weight', 'backbone.0.body.layer2.0.bn1.bias', 'backbone.0.body.layer2.0.bn1.running_mean', 'backbone.0.body.layer2.0.bn1.running_var', 'backbone.0.body.layer2.0.conv2.weight', 'backbone.0.body.layer2.0.bn2.weight', 'backbone.0.body.layer2.0.bn2.bias', 'backbone.0.body.layer2.0.bn2.running_mean', 'backbone.0.body.layer2.0.bn2.running_var', 'backbone.0.body.layer2.0.conv3.weight', 'backbone.0.body.layer2.0.bn3.weight', 'backbone.0.body.layer2.0.bn3.bias', 'backbone.0.body.layer2.0.bn3.running_mean', 'backbone.0.body.layer2.0.bn3.running_var', 'backbone.0.body.layer2.0.downsample.0.weight', 'backbone.0.body.layer2.0.downsample.1.weight', 'backbone.0.body.layer2.0.downsample.1.bias', 'backbone.0.body.layer2.0.downsample.1.running_mean', 'backbone.0.body.layer2.0.downsample.1.running_var', 'backbone.0.body.layer2.1.conv1.weight', 'backbone.0.body.layer2.1.bn1.weight', 'backbone.0.body.layer2.1.bn1.bias', 'backbone.0.body.layer2.1.bn1.running_mean', 'backbone.0.body.layer2.1.bn1.running_var', 'backbone.0.body.layer2.1.conv2.weight', 'backbone.0.body.layer2.1.bn2.weight', 'backbone.0.body.layer2.1.bn2.bias', 'backbone.0.body.layer2.1.bn2.running_mean', 'backbone.0.body.layer2.1.bn2.running_var', 'backbone.0.body.layer2.1.conv3.weight', 'backbone.0.body.layer2.1.bn3.weight', 'backbone.0.body.layer2.1.bn3.bias', 'backbone.0.body.layer2.1.bn3.running_mean', 'backbone.0.body.layer2.1.bn3.running_var', 'backbone.0.body.layer2.2.conv1.weight', 'backbone.0.body.layer2.2.bn1.weight', 'backbone.0.body.layer2.2.bn1.bias', 'backbone.0.body.layer2.2.bn1.running_mean', 'backbone.0.body.layer2.2.bn1.running_var', 'backbone.0.body.layer2.2.conv2.weight', 'backbone.0.body.layer2.2.bn2.weight', 'backbone.0.body.layer2.2.bn2.bias', 'backbone.0.body.layer2.2.bn2.running_mean', 'backbone.0.body.layer2.2.bn2.running_var', 'backbone.0.body.layer2.2.conv3.weight', 'backbone.0.body.layer2.2.bn3.weight', 'backbone.0.body.layer2.2.bn3.bias', 'backbone.0.body.layer2.2.bn3.running_mean', 'backbone.0.body.layer2.2.bn3.running_var', 'backbone.0.body.layer2.3.conv1.weight', 'backbone.0.body.layer2.3.bn1.weight', 'backbone.0.body.layer2.3.bn1.bias', 'backbone.0.body.layer2.3.bn1.running_mean', 'backbone.0.body.layer2.3.bn1.running_var', 'backbone.0.body.layer2.3.conv2.weight', 'backbone.0.body.layer2.3.bn2.weight', 'backbone.0.body.layer2.3.bn2.bias', 'backbone.0.body.layer2.3.bn2.running_mean', 'backbone.0.body.layer2.3.bn2.running_var', 'backbone.0.body.layer2.3.conv3.weight', 'backbone.0.body.layer2.3.bn3.weight', 'backbone.0.body.layer2.3.bn3.bias', 'backbone.0.body.layer2.3.bn3.running_mean', 'backbone.0.body.layer2.3.bn3.running_var', 'backbone.0.body.layer3.0.conv1.weight', 'backbone.0.body.layer3.0.bn1.weight', 'backbone.0.body.layer3.0.bn1.bias', 'backbone.0.body.layer3.0.bn1.running_mean', 'backbone.0.body.layer3.0.bn1.running_var', 'backbone.0.body.layer3.0.conv2.weight', 'backbone.0.body.layer3.0.bn2.weight', 'backbone.0.body.layer3.0.bn2.bias', 'backbone.0.body.layer3.0.bn2.running_mean', 'backbone.0.body.layer3.0.bn2.running_var', 'backbone.0.body.layer3.0.conv3.weight', 'backbone.0.body.layer3.0.bn3.weight', 'backbone.0.body.layer3.0.bn3.bias', 'backbone.0.body.layer3.0.bn3.running_mean', 'backbone.0.body.layer3.0.bn3.running_var', 'backbone.0.body.layer3.0.downsample.0.weight', 'backbone.0.body.layer3.0.downsample.1.weight', 'backbone.0.body.layer3.0.downsample.1.bias', 'backbone.0.body.layer3.0.downsample.1.running_mean', 'backbone.0.body.layer3.0.downsample.1.running_var', 'backbone.0.body.layer3.1.conv1.weight', 'backbone.0.body.layer3.1.bn1.weight', 'backbone.0.body.layer3.1.bn1.bias', 'backbone.0.body.layer3.1.bn1.running_mean', 'backbone.0.body.layer3.1.bn1.running_var', 'backbone.0.body.layer3.1.conv2.weight', 'backbone.0.body.layer3.1.bn2.weight', 'backbone.0.body.layer3.1.bn2.bias', 'backbone.0.body.layer3.1.bn2.running_mean', 'backbone.0.body.layer3.1.bn2.running_var', 'backbone.0.body.layer3.1.conv3.weight', 'backbone.0.body.layer3.1.bn3.weight', 'backbone.0.body.layer3.1.bn3.bias', 'backbone.0.body.layer3.1.bn3.running_mean', 'backbone.0.body.layer3.1.bn3.running_var', 'backbone.0.body.layer3.2.conv1.weight', 'backbone.0.body.layer3.2.bn1.weight', 'backbone.0.body.layer3.2.bn1.bias', 'backbone.0.body.layer3.2.bn1.running_mean', 'backbone.0.body.layer3.2.bn1.running_var', 'backbone.0.body.layer3.2.conv2.weight', 'backbone.0.body.layer3.2.bn2.weight', 'backbone.0.body.layer3.2.bn2.bias', 'backbone.0.body.layer3.2.bn2.running_mean', 'backbone.0.body.layer3.2.bn2.running_var', 'backbone.0.body.layer3.2.conv3.weight', 'backbone.0.body.layer3.2.bn3.weight', 'backbone.0.body.layer3.2.bn3.bias', 'backbone.0.body.layer3.2.bn3.running_mean', 'backbone.0.body.layer3.2.bn3.running_var', 'backbone.0.body.layer3.3.conv1.weight', 'backbone.0.body.layer3.3.bn1.weight', 'backbone.0.body.layer3.3.bn1.bias', 'backbone.0.body.layer3.3.bn1.running_mean', 'backbone.0.body.layer3.3.bn1.running_var', 'backbone.0.body.layer3.3.conv2.weight', 'backbone.0.body.layer3.3.bn2.weight', 'backbone.0.body.layer3.3.bn2.bias', 'backbone.0.body.layer3.3.bn2.running_mean', 'backbone.0.body.layer3.3.bn2.running_var', 'backbone.0.body.layer3.3.conv3.weight', 'backbone.0.body.layer3.3.bn3.weight', 'backbone.0.body.layer3.3.bn3.bias', 'backbone.0.body.layer3.3.bn3.running_mean', 'backbone.0.body.layer3.3.bn3.running_var', 'backbone.0.body.layer3.4.conv1.weight', 'backbone.0.body.layer3.4.bn1.weight', 'backbone.0.body.layer3.4.bn1.bias', 'backbone.0.body.layer3.4.bn1.running_mean', 'backbone.0.body.layer3.4.bn1.running_var', 'backbone.0.body.layer3.4.conv2.weight', 'backbone.0.body.layer3.4.bn2.weight', 'backbone.0.body.layer3.4.bn2.bias', 'backbone.0.body.layer3.4.bn2.running_mean', 'backbone.0.body.layer3.4.bn2.running_var', 'backbone.0.body.layer3.4.conv3.weight', 'backbone.0.body.layer3.4.bn3.weight', 'backbone.0.body.layer3.4.bn3.bias', 'backbone.0.body.layer3.4.bn3.running_mean', 'backbone.0.body.layer3.4.bn3.running_var', 'backbone.0.body.layer3.5.conv1.weight', 'backbone.0.body.layer3.5.bn1.weight', 'backbone.0.body.layer3.5.bn1.bias', 'backbone.0.body.layer3.5.bn1.running_mean', 'backbone.0.body.layer3.5.bn1.running_var', 'backbone.0.body.layer3.5.conv2.weight', 'backbone.0.body.layer3.5.bn2.weight', 'backbone.0.body.layer3.5.bn2.bias', 'backbone.0.body.layer3.5.bn2.running_mean', 'backbone.0.body.layer3.5.bn2.running_var', 'backbone.0.body.layer3.5.conv3.weight', 'backbone.0.body.layer3.5.bn3.weight', 'backbone.0.body.layer3.5.bn3.bias', 'backbone.0.body.layer3.5.bn3.running_mean', 'backbone.0.body.layer3.5.bn3.running_var', 'backbone.0.body.layer4.0.conv1.weight', 'backbone.0.body.layer4.0.bn1.weight', 'backbone.0.body.layer4.0.bn1.bias', 'backbone.0.body.layer4.0.bn1.running_mean', 'backbone.0.body.layer4.0.bn1.running_var', 'backbone.0.body.layer4.0.conv2.weight', 'backbone.0.body.layer4.0.bn2.weight', 'backbone.0.body.layer4.0.bn2.bias', 'backbone.0.body.layer4.0.bn2.running_mean', 'backbone.0.body.layer4.0.bn2.running_var', 'backbone.0.body.layer4.0.conv3.weight', 'backbone.0.body.layer4.0.bn3.weight', 'backbone.0.body.layer4.0.bn3.bias', 'backbone.0.body.layer4.0.bn3.running_mean', 'backbone.0.body.layer4.0.bn3.running_var', 'backbone.0.body.layer4.0.downsample.0.weight', 'backbone.0.body.layer4.0.downsample.1.weight', 'backbone.0.body.layer4.0.downsample.1.bias', 'backbone.0.body.layer4.0.downsample.1.running_mean', 'backbone.0.body.layer4.0.downsample.1.running_var', 'backbone.0.body.layer4.1.conv1.weight', 'backbone.0.body.layer4.1.bn1.weight', 'backbone.0.body.layer4.1.bn1.bias', 'backbone.0.body.layer4.1.bn1.running_mean', 'backbone.0.body.layer4.1.bn1.running_var', 'backbone.0.body.layer4.1.conv2.weight', 'backbone.0.body.layer4.1.bn2.weight', 'backbone.0.body.layer4.1.bn2.bias', 'backbone.0.body.layer4.1.bn2.running_mean', 'backbone.0.body.layer4.1.bn2.running_var', 'backbone.0.body.layer4.1.conv3.weight', 'backbone.0.body.layer4.1.bn3.weight', 'backbone.0.body.layer4.1.bn3.bias', 'backbone.0.body.layer4.1.bn3.running_mean', 'backbone.0.body.layer4.1.bn3.running_var', 'backbone.0.body.layer4.2.conv1.weight', 'backbone.0.body.layer4.2.bn1.weight', 'backbone.0.body.layer4.2.bn1.bias', 'backbone.0.body.layer4.2.bn1.running_mean', 'backbone.0.body.layer4.2.bn1.running_var', 'backbone.0.body.layer4.2.conv2.weight', 'backbone.0.body.layer4.2.bn2.weight', 'backbone.0.body.layer4.2.bn2.bias', 'backbone.0.body.layer4.2.bn2.running_mean', 'backbone.0.body.layer4.2.bn2.running_var', 'backbone.0.body.layer4.2.conv3.weight', 'backbone.0.body.layer4.2.bn3.weight', 'backbone.0.body.layer4.2.bn3.bias', 'backbone.0.body.layer4.2.bn3.running_mean', 'backbone.0.body.layer4.2.bn3.running_var'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint[\"model\"]\n",
    "checkpoint[\"model\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ad9aa8e5-b610-4a69-8094-489318f6ce6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"model\"]['class_embed.weight'][91].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798f655",
   "metadata": {},
   "source": [
    "# Modify model architecture \n",
    "\n",
    "The original classification head (`model.class_embed`) in DETR is `Linear(256, 92)`, meaning:\n",
    "\n",
    "\n",
    "`model.class_embed.weight.shape = torch.Size([92, 256])` \\\n",
    "`model.class_embed.bias.shape =torch.Size([92])`\n",
    "\n",
    "\n",
    "Our newly added classification head has two layers:\n",
    "\n",
    "- `nn.Linear(256, 128)`: This transforms the 256-dimensional decoder output to 128 dimensions.\n",
    "\n",
    "- `nn.Linear(128, 92)`: This maps the 128-dimensional intermediate features to 92 class logits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e931e19",
   "metadata": {},
   "source": [
    "# Modify weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99dfd65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check OLD classification head size:\n",
    "# checkpoint[\"model\"][\"class_embed.weight\"].shape  # torch.Size([92, 256])\n",
    "# checkpoint[\"model\"][\"class_embed.bias\"].shape  # torch.Size([92])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98b923d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "(class_embed): Sequential(\n",
    "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=128, out_features=92, bias=True)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "#COPY bias for layer 2 because it mataches\n",
    "kept_bias_vector = checkpoint[\"model\"][\"class_embed.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79f95917",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = model.class_embed[0].in_features #256\n",
    "new_layer_dim =  model.class_embed[0].out_features #128\n",
    "output_dim = model.class_embed[2].out_features #92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28d4efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove OLD classification head architecture parameters:\n",
    "del checkpoint[\"model\"][\"class_embed.weight\"]\n",
    "del checkpoint[\"model\"][\"class_embed.bias\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0926def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint[\"model\"][\"class_embed.0.weight\"] = nn.init.xavier_uniform_(\n",
    "    torch.empty(new_layer_dim, hidden_dim)\n",
    ")\n",
    "checkpoint[\"model\"][\"class_embed.0.bias\"] = torch.zeros(new_layer_dim)\n",
    "\n",
    "checkpoint[\"model\"][\"class_embed.2.weight\"] = nn.init.xavier_uniform_(\n",
    "    torch.empty(output_dim, new_layer_dim)\n",
    ")\n",
    "checkpoint[\"model\"][\"class_embed.2.bias\"] = kept_bias_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e92eb9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0984, -0.0830,  0.1219,  ..., -0.0645, -0.1471, -0.0400],\n",
       "        [-0.0561,  0.1276,  0.0486,  ...,  0.1135, -0.0857,  0.1623],\n",
       "        [ 0.1393,  0.0384, -0.1296,  ...,  0.1426,  0.0405,  0.0259],\n",
       "        ...,\n",
       "        [ 0.0399, -0.1557, -0.0984,  ..., -0.0087, -0.0694,  0.0376],\n",
       "        [-0.1121, -0.0906,  0.0083,  ..., -0.0690,  0.0472,  0.1281],\n",
       "        [-0.0671,  0.1335, -0.0384,  ..., -0.1646,  0.0270, -0.1368]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint['model'][\"class_embed.2.weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a101d2",
   "metadata": {},
   "source": [
    "## Apply changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb29e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified checkpoint saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the modified checkpoint\n",
    "torch.save(checkpoint, \"detr-r50-modifhead-128fc92fc.pth\")\n",
    "\n",
    "print(\"Modified checkpoint saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6fc4c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test:\n",
    "# checkpoint[\"model\"]\n",
    "model.load_state_dict(checkpoint['model'], strict=False)  #### Modified by Amir: , strict=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c305ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mclass_embed[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.class_embed[2].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd876b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test2:\n",
    "## load the new checkpoint into the modified detr\n",
    "checkpoint = torch.load('detr-r50-modifhead-128fc92fc.pth', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'], strict=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ccd7618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for param in model.class_embed[2].parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afac17f",
   "metadata": {},
   "source": [
    "# KITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34712473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anazeri/.conda/envs/Detr_env1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = get_args_parser()\n",
    "    args = parser.parse_args(['--new_layer_dim', '128', '--device', 'cpu', '--num_classes', '9'])\n",
    "    model, _, _ = build_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738314ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.class_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b99fcee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained weights\n",
    "checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',        \n",
    "            # url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n",
    "            # url='https://dl.fbaipublicfiles.com/detr/detr-r101-2c7b67e5.pth',\n",
    "            # url='https://dl.fbaipublicfiles.com/detr/detr-r101-dc5-a2e86def.pth',\n",
    "            map_location='cpu',\n",
    "            check_hash=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305d1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove OLD classification head architecture parameters:\n",
    "del checkpoint[\"model\"][\"class_embed.weight\"]\n",
    "del checkpoint[\"model\"][\"class_embed.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced8e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = model.class_embed[0].in_features #256\n",
    "new_layer_dim =  model.class_embed[0].out_features #128\n",
    "output_dim = model.class_embed[2].out_features #10\n",
    "\n",
    "\n",
    "checkpoint[\"model\"][\"class_embed.0.weight\"] = nn.init.xavier_uniform_(\n",
    "    torch.empty(new_layer_dim, hidden_dim)\n",
    ")\n",
    "checkpoint[\"model\"][\"class_embed.0.bias\"] = torch.zeros(new_layer_dim)\n",
    "\n",
    "checkpoint[\"model\"][\"class_embed.2.weight\"] = nn.init.xavier_uniform_(\n",
    "    torch.empty(output_dim, new_layer_dim)\n",
    ")\n",
    "checkpoint[\"model\"][\"class_embed.2.bias\"] = torch.zeros(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4570b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified checkpoint saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the modified checkpoint\n",
    "torch.save(checkpoint, \"detr-r50-KITTI-modifhead-128fc92fc.pth\")\n",
    "print(\"Modified checkpoint saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff81382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model'], strict=False)  #### Modified by Amir: , strict=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-asthma",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Our dataset should be loadable as a COCO format\n",
    "\n",
    "This allows us to use the pycocotools to load the data dict for the main python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "laden-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"coco\" # alternatively, implement your own coco-type dataset loader in datasets and add this \"key\" to datasets/__init__.py\n",
    "\n",
    "dataDir='/home/anazeri/fiftyone/kitti_coco/' # should lead to a directory with a train2017 and val2017 folder as well as an annotations folder\n",
    "num_classes = 9+1 # this int should be the actual number of classes + 1 (for no class)\n",
    "\n",
    "# \n",
    "\n",
    "outDir = 'outputs'\n",
    "resume = \"detr-r50_no-class-head.pth\" if pretrained else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98908ba9-19e5-4416-8646-5b0e788a0a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "python main.py \\\n",
    "  --dataset_file $\"coco\" \\\n",
    "  --coco_path $'/home/anazeri/fiftyone/kitti_coco/' \\\n",
    "  --output_dir $'outputs' \\\n",
    "  --resume $\"detr-r50_no-class-head.pth\" \\\n",
    "  --num_classes $num_classes \\\n",
    "  --lr 1e-5 \\\n",
    "  --lr_backbone 1e-6 \\\n",
    "  --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-version",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We use the main.py script to run our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hazardous-possibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/anazeri/detr_finetune/main.py\", line 10, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "  --dataset_file $dataset_file \\\n",
    "  --coco_path $dataDir \\\n",
    "  --output_dir $outDir \\\n",
    "  --resume $resume \\\n",
    "  --num_classes $num_classes \\\n",
    "  --lr 1e-5 \\\n",
    "  --lr_backbone 1e-6 \\\n",
    "  --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-sister",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Quick and easy overview of the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.plot_utils import plot_logs\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "log_directory = [Path(outDir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_of_interest = (\n",
    "    'loss',\n",
    "    'mAP',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_of_interest = (\n",
    "    'loss_ce',\n",
    "    'loss_bbox',\n",
    "    'loss_giou',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_of_interest = (\n",
    "    'class_error',\n",
    "    'cardinality_error_unscaled',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Detr_env1",
   "language": "python",
   "name": "detr_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
